{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT Rounding Mode Analysis\n",
    "\n",
    "**Hypothesis**: Systematic rounding bias (floor/ceil) is easier to optimize against than nearest rounding's \"random flipping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../runs/experiment_20260118_151239/summary.csv')\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nRounding modes: {df['rounding'].unique()}\")\n",
    "print(f\"Widths: {sorted(df['width'].unique())}\")\n",
    "print(f\"Depths: {sorted(df['depth'].unique())}\")\n",
    "print(f\"Seeds: {sorted(df['seed'].unique())}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Comparison: Rounding Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean accuracy by rounding mode\n",
    "summary = df.groupby('rounding')['final_test_acc'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "summary = summary.round(4)\n",
    "print(\"Overall accuracy by rounding mode:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "order = ['baseline', 'nearest', 'floor', 'ceil']\n",
    "sns.boxplot(data=df, x='rounding', y='final_test_acc', order=order, ax=axes[0])\n",
    "axes[0].set_title('Test Accuracy Distribution by Rounding Mode')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_xlabel('Rounding Mode')\n",
    "\n",
    "# Bar plot with error bars\n",
    "means = df.groupby('rounding')['final_test_acc'].mean().reindex(order)\n",
    "stds = df.groupby('rounding')['final_test_acc'].std().reindex(order)\n",
    "axes[1].bar(order, means, yerr=stds, capsize=5, alpha=0.7, color=['gray', 'blue', 'green', 'red'])\n",
    "axes[1].set_title('Mean Test Accuracy (± std)')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_xlabel('Rounding Mode')\n",
    "axes[1].set_ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired comparison: for each (seed, width, depth), compare rounding modes\n",
    "# Pivot to get each config as a row\n",
    "pivot = df.pivot_table(index=['seed', 'width', 'depth'], columns='rounding', values='final_test_acc')\n",
    "\n",
    "print(\"Paired t-tests (vs nearest):\")\n",
    "for mode in ['floor', 'ceil', 'baseline']:\n",
    "    t_stat, p_val = stats.ttest_rel(pivot[mode], pivot['nearest'])\n",
    "    diff = (pivot[mode] - pivot['nearest']).mean()\n",
    "    print(f\"  {mode} vs nearest: diff={diff:+.4f}, t={t_stat:.2f}, p={p_val:.4f}\")\n",
    "\n",
    "print(\"\\nPaired t-tests (floor vs ceil):\")\n",
    "t_stat, p_val = stats.ttest_rel(pivot['floor'], pivot['ceil'])\n",
    "diff = (pivot['floor'] - pivot['ceil']).mean()\n",
    "print(f\"  floor vs ceil: diff={diff:+.4f}, t={t_stat:.2f}, p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy by Model Capacity (Width × Depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps: mean accuracy by width × depth for each rounding mode\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "for i, mode in enumerate(['baseline', 'nearest', 'floor', 'ceil']):\n",
    "    subset = df[df['rounding'] == mode]\n",
    "    heatmap_data = subset.pivot_table(index='depth', columns='width', values='final_test_acc', aggfunc='mean')\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0.5, vmax=1.0, ax=axes[i])\n",
    "    axes[i].set_title(f'{mode}')\n",
    "    axes[i].set_xlabel('Width')\n",
    "    axes[i].set_ylabel('Depth' if i == 0 else '')\n",
    "\n",
    "plt.suptitle('Mean Test Accuracy by Width × Depth', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference heatmaps: how much better/worse than nearest?\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "nearest_pivot = df[df['rounding'] == 'nearest'].pivot_table(\n",
    "    index='depth', columns='width', values='final_test_acc', aggfunc='mean')\n",
    "\n",
    "for i, mode in enumerate(['baseline', 'floor', 'ceil']):\n",
    "    mode_pivot = df[df['rounding'] == mode].pivot_table(\n",
    "        index='depth', columns='width', values='final_test_acc', aggfunc='mean')\n",
    "    diff = mode_pivot - nearest_pivot\n",
    "    sns.heatmap(diff, annot=True, fmt='+.2f', cmap='RdBu', center=0, vmin=-0.2, vmax=0.2, ax=axes[i])\n",
    "    axes[i].set_title(f'{mode} - nearest')\n",
    "    axes[i].set_xlabel('Width')\n",
    "    axes[i].set_ylabel('Depth' if i == 0 else '')\n",
    "\n",
    "plt.suptitle('Accuracy Difference vs Nearest Rounding (positive = better)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Out Collapsed Models\n",
    "\n",
    "Models with very small width often collapse to random (50%). Let's analyze only models that actually learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a model \"learned\" if accuracy > 0.55\n",
    "df_learned = df[df['final_test_acc'] > 0.55].copy()\n",
    "print(f\"Total runs: {len(df)}\")\n",
    "print(f\"Learned (acc > 0.55): {len(df_learned)} ({100*len(df_learned)/len(df):.1f}%)\")\n",
    "print(f\"\\nBreakdown by rounding mode:\")\n",
    "print(df.groupby('rounding').apply(lambda x: (x['final_test_acc'] > 0.55).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do comparison on learned models only\n",
    "print(\"Learned models only - accuracy by rounding mode:\")\n",
    "summary_learned = df_learned.groupby('rounding')['final_test_acc'].agg(['mean', 'std', 'count'])\n",
    "summary_learned.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis by Width Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group widths: small (4-8), medium (16-32), large (64-128)\n",
    "def width_category(w):\n",
    "    if w <= 8: return 'small (4-8)'\n",
    "    elif w <= 32: return 'medium (16-32)'\n",
    "    else: return 'large (64-128)'\n",
    "\n",
    "df['width_cat'] = df['width'].apply(width_category)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "order = ['baseline', 'nearest', 'floor', 'ceil']\n",
    "\n",
    "for i, cat in enumerate(['small (4-8)', 'medium (16-32)', 'large (64-128)']):\n",
    "    subset = df[df['width_cat'] == cat]\n",
    "    sns.boxplot(data=subset, x='rounding', y='final_test_acc', order=order, ax=axes[i])\n",
    "    axes[i].set_title(f'Width: {cat}')\n",
    "    axes[i].set_ylim(0.4, 1.0)\n",
    "    \n",
    "plt.suptitle('Accuracy by Rounding Mode, Split by Model Width', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats for each width category\n",
    "print(\"Mean accuracy by rounding mode and width category:\")\n",
    "pd.pivot_table(df, index='width_cat', columns='rounding', values='final_test_acc', aggfunc='mean').round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variance Analysis: Which rounding mode is more consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each (width, depth), compute std across seeds for each rounding mode\n",
    "variance_by_config = df.groupby(['width', 'depth', 'rounding'])['final_test_acc'].std().reset_index()\n",
    "variance_by_config.columns = ['width', 'depth', 'rounding', 'std_across_seeds']\n",
    "\n",
    "print(\"Mean std across seeds by rounding mode:\")\n",
    "print(variance_by_config.groupby('rounding')['std_across_seeds'].mean().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.boxplot(data=variance_by_config, x='rounding', y='std_across_seeds', order=order)\n",
    "ax.set_title('Consistency: Std of Accuracy Across Seeds')\n",
    "ax.set_ylabel('Std (lower = more consistent)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall winner\n",
    "means = df.groupby('rounding')['final_test_acc'].mean()\n",
    "print(f\"\\nOverall mean accuracy:\")\n",
    "for mode in order:\n",
    "    print(f\"  {mode:10s}: {means[mode]:.4f}\")\n",
    "\n",
    "print(f\"\\nBest overall: {means.idxmax()} ({means.max():.4f})\")\n",
    "\n",
    "# QAT comparison (excluding baseline)\n",
    "qat_modes = ['nearest', 'floor', 'ceil']\n",
    "qat_means = means[qat_modes]\n",
    "print(f\"\\nBest QAT mode: {qat_means.idxmax()} ({qat_means.max():.4f})\")\n",
    "\n",
    "# Hypothesis test\n",
    "print(f\"\\nHypothesis: systematic bias (floor/ceil) beats nearest\")\n",
    "floor_vs_nearest = (pivot['floor'] - pivot['nearest']).mean()\n",
    "ceil_vs_nearest = (pivot['ceil'] - pivot['nearest']).mean()\n",
    "print(f\"  floor - nearest: {floor_vs_nearest:+.4f}\")\n",
    "print(f\"  ceil - nearest:  {ceil_vs_nearest:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-aware-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
