{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b47c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Uniform Diagonal Weights\n",
    "# ======================================\n",
    "# \n",
    "# Setup:\n",
    "# - 3×1 input \"image\": [10, 20, 30]\n",
    "# - Each layer is a diagonal matrix with identical entries: w·I\n",
    "# - No activations\n",
    "# - Track error shapes mapped back to input space\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from itertools import product\n",
    "\n",
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input\n",
    "x_input = np.array([10.0, 20.0, 30.0])\n",
    "\n",
    "# Quantization parameters\n",
    "bits = 8\n",
    "delta = 1.0 / (2 ** (bits - 1))  # Quantization step for weights\n",
    "\n",
    "# Network: 4 layers, each with uniform diagonal weights\n",
    "# True weights (before quantization)\n",
    "true_weights = [0.8, 1.2, 0.9, 1.1]\n",
    "\n",
    "def quantize(w, delta):\n",
    "    \"\"\"Quantize a weight to nearest grid point\"\"\"\n",
    "    return np.round(w / delta) * delta\n",
    "\n",
    "# Quantized weights\n",
    "quant_weights = [quantize(w, delta) for w in true_weights]\n",
    "weight_errors = [qw - tw for qw, tw in zip(quant_weights, true_weights)]\n",
    "\n",
    "print(\"Layer configurations:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (tw, qw, we) in enumerate(zip(true_weights, quant_weights, weight_errors)):\n",
    "    print(f\"Layer {i+1}: true={tw:.4f}, quantized={qw:.6f}, error={we:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core computation: trace values and errors through layers\n",
    "# ============================================================\n",
    "\n",
    "def trace_through_network(x, true_weights, quant_weights):\n",
    "    \"\"\"\n",
    "    Trace a value through the network, computing:\n",
    "    - The quantized output at each layer\n",
    "    - The error introduced at each layer\n",
    "    - The cumulative error\n",
    "    \"\"\"\n",
    "    history = [{\n",
    "        'layer': 0,\n",
    "        'value': x.copy(),\n",
    "        'error_this_layer': np.zeros_like(x),\n",
    "        'cumulative_error': np.zeros_like(x),\n",
    "        'cumulative_error_in_input_space': np.zeros_like(x)\n",
    "    }]\n",
    "    \n",
    "    val_quant = x.copy()  # Value in quantized network\n",
    "    val_true = x.copy()   # Value in true (FP) network\n",
    "    \n",
    "    # For mapping back to input space, track cumulative weight product\n",
    "    cumulative_weight_product = 1.0\n",
    "    \n",
    "    for i, (w_true, w_quant) in enumerate(zip(true_weights, quant_weights)):\n",
    "        # True network\n",
    "        val_true = w_true * val_true\n",
    "        \n",
    "        # Quantized network\n",
    "        val_quant_new = w_quant * val_quant\n",
    "        \n",
    "        # Error introduced THIS layer: (w_quant - w_true) * val_quant\n",
    "        # Note: we use val_quant because that's what the quantized network sees\n",
    "        w_error = w_quant - w_true\n",
    "        error_this_layer = w_error * val_quant\n",
    "        \n",
    "        # Update\n",
    "        val_quant = val_quant_new\n",
    "        \n",
    "        # Cumulative error in output space\n",
    "        cumulative_error = val_quant - val_true\n",
    "        \n",
    "        # Map error back to input space\n",
    "        # The error at layer i, mapped to input space, is divided by product of all weights up to i\n",
    "        cumulative_weight_product *= w_quant\n",
    "        error_in_input_space = cumulative_error / cumulative_weight_product\n",
    "        \n",
    "        history.append({\n",
    "            'layer': i + 1,\n",
    "            'value_quant': val_quant.copy(),\n",
    "            'value_true': val_true.copy(),\n",
    "            'error_this_layer': error_this_layer.copy(),\n",
    "            'cumulative_error': cumulative_error.copy(),\n",
    "            'cumulative_error_in_input_space': error_in_input_space.copy(),\n",
    "            'weight_true': w_true,\n",
    "            'weight_quant': w_quant,\n",
    "            'cumulative_weight_product': cumulative_weight_product\n",
    "        })\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "history = trace_through_network(x_input, true_weights, quant_weights)\n",
    "\n",
    "print(\"\\n\\nValue and error propagation:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Layer':<8} {'Value (quant)':<25} {'Cumulative Error':<25} {'Error in Input Space':<25}\")\n",
    "print(\"-\" * 80)\n",
    "for h in history:\n",
    "    layer = h['layer']\n",
    "    if layer == 0:\n",
    "        val_str = str(h['value'])\n",
    "        err_str = \"[0, 0, 0]\"\n",
    "        err_input_str = \"[0, 0, 0]\"\n",
    "    else:\n",
    "        val_str = f\"[{h['value_quant'][0]:.3f}, {h['value_quant'][1]:.3f}, {h['value_quant'][2]:.3f}]\"\n",
    "        err_str = f\"[{h['cumulative_error'][0]:.4f}, {h['cumulative_error'][1]:.4f}, {h['cumulative_error'][2]:.4f}]\"\n",
    "        err_input_str = f\"[{h['cumulative_error_in_input_space'][0]:.4f}, {h['cumulative_error_in_input_space'][1]:.4f}, {h['cumulative_error_in_input_space'][2]:.4f}]\"\n",
    "    print(f\"{layer:<8} {val_str:<25} {err_str:<25} {err_input_str:<25}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute error REGIONS (not just single errors)\n",
    "# ============================================================\n",
    "\n",
    "def compute_error_box_at_layer(x_at_layer, delta):\n",
    "    \"\"\"\n",
    "    The error from quantizing weights at this layer.\n",
    "    \n",
    "    Each weight has error in [-delta/2, +delta/2].\n",
    "    The output error is weight_error * x, so it's in:\n",
    "    [-delta/2 * |x|, +delta/2 * |x|] for each dimension.\n",
    "    \n",
    "    Returns the half-widths of the error box.\n",
    "    \"\"\"\n",
    "    return (delta / 2) * np.abs(x_at_layer)\n",
    "\n",
    "\n",
    "def compute_error_boxes_through_network(x, quant_weights, delta):\n",
    "    \"\"\"\n",
    "    Compute the error box contributed by each layer,\n",
    "    all mapped back to input space.\n",
    "    \n",
    "    Returns list of half-widths for each layer's contribution.\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    \n",
    "    val = x.copy()\n",
    "    cumulative_weight = 1.0\n",
    "    \n",
    "    for i, w in enumerate(quant_weights):\n",
    "        # Error box at this layer (in layer's output space)\n",
    "        box_at_layer = compute_error_box_at_layer(val, delta)\n",
    "        \n",
    "        # Map back to input space: divide by cumulative weight so far\n",
    "        # (The current layer's error is in output space of this layer,\n",
    "        # which is input space scaled by cumulative_weight * w)\n",
    "        cumulative_weight_after = cumulative_weight * w\n",
    "        box_in_input_space = box_at_layer / cumulative_weight_after\n",
    "        \n",
    "        boxes.append({\n",
    "            'layer': i + 1,\n",
    "            'box_half_widths_output_space': box_at_layer.copy(),\n",
    "            'box_half_widths_input_space': box_in_input_space.copy(),\n",
    "            'value_at_layer_input': val.copy(),\n",
    "            'cumulative_weight': cumulative_weight_after\n",
    "        })\n",
    "        \n",
    "        # Update value for next layer\n",
    "        val = w * val\n",
    "        cumulative_weight = cumulative_weight_after\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "\n",
    "boxes = compute_error_boxes_through_network(x_input, quant_weights, delta)\n",
    "\n",
    "print(\"\\n\\nError boxes per layer (mapped to input space):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Layer':<8} {'Box half-widths (input space)':<40} {'Value seen by layer':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for b in boxes:\n",
    "    hw = b['box_half_widths_input_space']\n",
    "    val = b['value_at_layer_input']\n",
    "    print(f\"{b['layer']:<8} [{hw[0]:.6f}, {hw[1]:.6f}, {hw[2]:.6f}]    [{val[0]:.2f}, {val[1]:.2f}, {val[2]:.2f}]\")\n",
    "\n",
    "# Minkowski sum: for axis-aligned boxes, just add the half-widths\n",
    "total_box_half_widths = np.zeros(3)\n",
    "for b in boxes:\n",
    "    total_box_half_widths += b['box_half_widths_input_space']\n",
    "\n",
    "print(f\"\\nTotal error box (Minkowski sum): [{total_box_half_widths[0]:.6f}, {total_box_half_widths[1]:.6f}, {total_box_half_widths[2]:.6f}]\")\n",
    "print(f\"Total error range per channel:\")\n",
    "for i, (hw, x) in enumerate(zip(total_box_half_widths, x_input)):\n",
    "    print(f\"  Channel {i} (input={x}): error in [{-hw:.6f}, {+hw:.6f}], i.e., ±{100*hw/x:.3f}% of input\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "\n",
    "def draw_box_3d(ax, center, half_widths, color, alpha=0.3, label=None):\n",
    "    \"\"\"Draw a 3D box centered at 'center' with given half-widths.\"\"\"\n",
    "    hw = half_widths\n",
    "    \n",
    "    # 8 vertices of the box\n",
    "    vertices = np.array(list(product([-1, 1], repeat=3))) * hw + center\n",
    "    \n",
    "    # 6 faces (each face is 4 vertices)\n",
    "    faces = [\n",
    "        [vertices[0], vertices[1], vertices[3], vertices[2]],  # bottom\n",
    "        [vertices[4], vertices[5], vertices[7], vertices[6]],  # top\n",
    "        [vertices[0], vertices[1], vertices[5], vertices[4]],  # front\n",
    "        [vertices[2], vertices[3], vertices[7], vertices[6]],  # back\n",
    "        [vertices[0], vertices[2], vertices[6], vertices[4]],  # left\n",
    "        [vertices[1], vertices[3], vertices[7], vertices[5]],  # right\n",
    "    ]\n",
    "    \n",
    "    ax.add_collection3d(Poly3DCollection(\n",
    "        faces, alpha=alpha, facecolor=color, edgecolor='black', linewidth=0.5\n",
    "    ))\n",
    "    \n",
    "    if label:\n",
    "        ax.text(center[0], center[1], center[2] + hw[2] * 1.2, label, fontsize=10)\n",
    "\n",
    "\n",
    "# Figure 1: Error boxes per layer in input space\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Individual error boxes from each layer\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(boxes)))\n",
    "offset = 0\n",
    "for i, (b, color) in enumerate(zip(boxes, colors)):\n",
    "    center = np.array([offset, offset, offset])  # Offset for visibility\n",
    "    hw = b['box_half_widths_input_space']\n",
    "    draw_box_3d(ax1, center, hw, color, alpha=0.5, label=f\"L{b['layer']}\")\n",
    "    offset += max(hw) * 2.5\n",
    "\n",
    "ax1.set_xlabel('Channel 0 (x=10)')\n",
    "ax1.set_ylabel('Channel 1 (x=20)')\n",
    "ax1.set_zlabel('Channel 2 (x=30)')\n",
    "ax1.set_title('Error boxes from each layer\\n(separated for visibility)')\n",
    "\n",
    "\n",
    "# Plot 2: Cumulative Minkowski sum\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "\n",
    "cumulative_hw = np.zeros(3)\n",
    "for i, (b, color) in enumerate(zip(boxes, colors)):\n",
    "    cumulative_hw = cumulative_hw + b['box_half_widths_input_space']\n",
    "    # Draw at same center, growing box\n",
    "    draw_box_3d(ax2, np.zeros(3), cumulative_hw, color, alpha=0.2)\n",
    "\n",
    "ax2.set_xlabel('Channel 0 (x=10)')\n",
    "ax2.set_ylabel('Channel 1 (x=20)')\n",
    "ax2.set_zlabel('Channel 2 (x=30)')\n",
    "ax2.set_title('Cumulative error box\\n(Minkowski sum, nested)')\n",
    "\n",
    "\n",
    "# Plot 3: Final error box with proportions\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "draw_box_3d(ax3, np.zeros(3), total_box_half_widths, 'red', alpha=0.4)\n",
    "\n",
    "# Mark the axes extents\n",
    "for i, (hw, label) in enumerate(zip(total_box_half_widths, ['x=10', 'x=20', 'x=30'])):\n",
    "    if i == 0:\n",
    "        ax3.plot([-hw, hw], [0, 0], [0, 0], 'b-', linewidth=3, label=f'Ch0 ({label}): ±{hw:.4f}')\n",
    "    elif i == 1:\n",
    "        ax3.plot([0, 0], [-hw, hw], [0, 0], 'g-', linewidth=3, label=f'Ch1 ({label}): ±{hw:.4f}')\n",
    "    else:\n",
    "        ax3.plot([0, 0], [0, 0], [-hw, hw], 'r-', linewidth=3, label=f'Ch2 ({label}): ±{hw:.4f}')\n",
    "\n",
    "ax3.set_xlabel('Channel 0')\n",
    "ax3.set_ylabel('Channel 1')\n",
    "ax3.set_zlabel('Channel 2')\n",
    "ax3.set_title('Final error box\\nNote: larger input → larger error')\n",
    "ax3.legend(loc='upper left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment1_error_boxes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: Error growth through layers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Per-layer contribution\n",
    "ax = axes[0]\n",
    "layer_nums = [b['layer'] for b in boxes]\n",
    "for ch in range(3):\n",
    "    contributions = [b['box_half_widths_input_space'][ch] for b in boxes]\n",
    "    ax.bar(np.array(layer_nums) + (ch - 1) * 0.25, contributions, width=0.25, \n",
    "           label=f'Channel {ch} (input={x_input[ch]:.0f})')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Error half-width (input space)')\n",
    "ax.set_title('Error contribution per layer')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative error\n",
    "ax = axes[1]\n",
    "for ch in range(3):\n",
    "    cumulative = np.cumsum([b['box_half_widths_input_space'][ch] for b in boxes])\n",
    "    ax.plot(layer_nums, cumulative, 'o-', linewidth=2, markersize=8, \n",
    "            label=f'Channel {ch} (input={x_input[ch]:.0f})')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Cumulative error half-width')\n",
    "ax.set_title('Cumulative error (Minkowski sum)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Error as percentage of input\n",
    "ax = axes[2]\n",
    "for ch in range(3):\n",
    "    cumulative = np.cumsum([b['box_half_widths_input_space'][ch] for b in boxes])\n",
    "    percentage = 100 * cumulative / x_input[ch]\n",
    "    ax.plot(layer_nums, percentage, 'o-', linewidth=2, markersize=8,\n",
    "            label=f'Channel {ch}')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Error as % of input value')\n",
    "ax.set_title('Relative error\\n(Note: same % for all channels!)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment1_error_growth.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Key observations\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ERROR SCALES WITH INPUT MAGNITUDE\n",
    "   - Channel 2 (input=30) has 3x the absolute error of Channel 0 (input=10)\n",
    "   - But the RELATIVE error (as % of input) is the same for all channels\n",
    "   - This is because we have uniform diagonal weights\n",
    "\n",
    "2. ERROR ACCUMULATES THROUGH LAYERS\n",
    "   - Each layer contributes its own error box\n",
    "   - Total error is the Minkowski sum (for axis-aligned boxes, just add half-widths)\n",
    "   - With 4 layers, error is ~4x a single layer (roughly, depends on weight magnitudes)\n",
    "\n",
    "3. WEIGHT MAGNITUDE MATTERS\n",
    "   - Weights > 1 amplify previous errors but also the current layer's error contribution\n",
    "   - Weights < 1 shrink previous errors\n",
    "   - The error from layer i, mapped to input space, is divided by the product of weights 1..i\n",
    "\n",
    "4. THE ERROR BOX IS AXIS-ALIGNED\n",
    "   - Because weights are diagonal, channels don't mix\n",
    "   - Error in each channel is independent\n",
    "   - This will change in Experiment 3 when we use non-diagonal weights\n",
    "\"\"\")\n",
    "\n",
    "# Verify the relative error is the same\n",
    "print(\"\\nVerification - relative error per channel:\")\n",
    "for ch in range(3):\n",
    "    rel_error = total_box_half_widths[ch] / x_input[ch]\n",
    "    print(f\"  Channel {ch}: {100*rel_error:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de964eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Non-Uniform Diagonal Weights\n",
    "# ==========================================\n",
    "# \n",
    "# Setup:\n",
    "# - Same 3×1 input \"image\": [10, 20, 30]\n",
    "# - Each layer is a diagonal matrix with DIFFERENT entries per channel\n",
    "# - No activations\n",
    "# - Track how different channels accumulate error at different rates\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from itertools import product\n",
    "\n",
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input\n",
    "x_input = np.array([10.0, 20.0, 30.0])\n",
    "\n",
    "# Quantization parameters\n",
    "bits = 8\n",
    "delta = 1.0 / (2 ** (bits - 1))\n",
    "\n",
    "# Network: 4 layers, each with NON-UNIFORM diagonal weights\n",
    "# Now each layer has different weights per channel\n",
    "# Shape: (n_layers, n_channels)\n",
    "true_weights = np.array([\n",
    "    [0.8, 1.2, 0.5],   # Layer 1: channel 1 amplified, channel 2 shrunk\n",
    "    [1.1, 0.7, 1.3],   # Layer 2: channel 0,2 amplified, channel 1 shrunk\n",
    "    [0.9, 1.1, 0.9],   # Layer 3: relatively uniform\n",
    "    [1.2, 0.8, 1.1],   # Layer 4: channel 0 amplified, channel 1 shrunk\n",
    "])\n",
    "\n",
    "n_layers, n_channels = true_weights.shape\n",
    "\n",
    "def quantize(w, delta):\n",
    "    \"\"\"Quantize weights to nearest grid point\"\"\"\n",
    "    return np.round(w / delta) * delta\n",
    "\n",
    "# Quantize all weights\n",
    "quant_weights = quantize(true_weights, delta)\n",
    "weight_errors = quant_weights - true_weights\n",
    "\n",
    "print(\"Layer configurations:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Layer':<8} {'Channel 0':<20} {'Channel 1':<20} {'Channel 2':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(n_layers):\n",
    "    ch0 = f\"{true_weights[i,0]:.3f}→{quant_weights[i,0]:.3f}\"\n",
    "    ch1 = f\"{true_weights[i,1]:.3f}→{quant_weights[i,1]:.3f}\"\n",
    "    ch2 = f\"{true_weights[i,2]:.3f}→{quant_weights[i,2]:.3f}\"\n",
    "    print(f\"{i+1:<8} {ch0:<20} {ch1:<20} {ch2:<20}\")\n",
    "\n",
    "# Compute cumulative weight products per channel\n",
    "cumulative_products = np.cumprod(quant_weights, axis=0)\n",
    "print(\"\\nCumulative weight products (determines total amplification):\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(n_layers):\n",
    "    print(f\"After layer {i+1}: [{cumulative_products[i,0]:.4f}, {cumulative_products[i,1]:.4f}, {cumulative_products[i,2]:.4f}]\")\n",
    "\n",
    "final_products = cumulative_products[-1]\n",
    "print(f\"\\nFinal amplification: Ch0={final_products[0]:.4f}x, Ch1={final_products[1]:.4f}x, Ch2={final_products[2]:.4f}x\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core computation: trace values and errors through layers\n",
    "# ============================================================\n",
    "\n",
    "def trace_through_network(x, true_weights, quant_weights):\n",
    "    \"\"\"\n",
    "    Trace a value through the network with per-channel weights.\n",
    "    \"\"\"\n",
    "    n_layers = true_weights.shape[0]\n",
    "    \n",
    "    history = [{\n",
    "        'layer': 0,\n",
    "        'value': x.copy(),\n",
    "        'cumulative_error': np.zeros_like(x),\n",
    "        'cumulative_error_in_input_space': np.zeros_like(x)\n",
    "    }]\n",
    "    \n",
    "    val_quant = x.copy()\n",
    "    val_true = x.copy()\n",
    "    cumulative_weight_product = np.ones_like(x)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        w_true = true_weights[i]\n",
    "        w_quant = quant_weights[i]\n",
    "        \n",
    "        # Update values (element-wise multiplication for diagonal weights)\n",
    "        val_true = w_true * val_true\n",
    "        val_quant_new = w_quant * val_quant\n",
    "        \n",
    "        val_quant = val_quant_new\n",
    "        cumulative_weight_product = cumulative_weight_product * w_quant\n",
    "        \n",
    "        # Cumulative error\n",
    "        cumulative_error = val_quant - val_true\n",
    "        \n",
    "        # Map back to input space (per-channel division)\n",
    "        error_in_input_space = cumulative_error / cumulative_weight_product\n",
    "        \n",
    "        history.append({\n",
    "            'layer': i + 1,\n",
    "            'value_quant': val_quant.copy(),\n",
    "            'value_true': val_true.copy(),\n",
    "            'weight_quant': w_quant.copy(),\n",
    "            'cumulative_error': cumulative_error.copy(),\n",
    "            'cumulative_error_in_input_space': error_in_input_space.copy(),\n",
    "            'cumulative_weight_product': cumulative_weight_product.copy()\n",
    "        })\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "history = trace_through_network(x_input, true_weights, quant_weights)\n",
    "\n",
    "print(\"\\n\\nValue and error propagation:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Layer':<7} {'Value (quant)':<30} {'Cumulative Error':<30} {'Error in Input Space':<30}\")\n",
    "print(\"-\" * 100)\n",
    "for h in history:\n",
    "    layer = h['layer']\n",
    "    if layer == 0:\n",
    "        val_str = f\"[{h['value'][0]:.2f}, {h['value'][1]:.2f}, {h['value'][2]:.2f}]\"\n",
    "        err_str = \"[0, 0, 0]\"\n",
    "        err_input_str = \"[0, 0, 0]\"\n",
    "    else:\n",
    "        val_str = f\"[{h['value_quant'][0]:.4f}, {h['value_quant'][1]:.4f}, {h['value_quant'][2]:.4f}]\"\n",
    "        err_str = f\"[{h['cumulative_error'][0]:.5f}, {h['cumulative_error'][1]:.5f}, {h['cumulative_error'][2]:.5f}]\"\n",
    "        err_input_str = f\"[{h['cumulative_error_in_input_space'][0]:.5f}, {h['cumulative_error_in_input_space'][1]:.5f}, {h['cumulative_error_in_input_space'][2]:.5f}]\"\n",
    "    print(f\"{layer:<7} {val_str:<30} {err_str:<30} {err_input_str:<30}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute error REGIONS per layer\n",
    "# ============================================================\n",
    "\n",
    "def compute_error_boxes_through_network(x, quant_weights, delta):\n",
    "    \"\"\"\n",
    "    Compute error boxes with per-channel weights.\n",
    "    \"\"\"\n",
    "    n_layers = quant_weights.shape[0]\n",
    "    boxes = []\n",
    "    \n",
    "    val = x.copy()\n",
    "    cumulative_weight = np.ones_like(x)\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        w = quant_weights[i]\n",
    "        \n",
    "        # Error box at this layer (in output space)\n",
    "        # Error = delta_w * val, and delta_w in [-delta/2, delta/2]\n",
    "        box_at_layer = (delta / 2) * np.abs(val)\n",
    "        \n",
    "        # Map to input space: divide by cumulative weight (per channel)\n",
    "        cumulative_weight_after = cumulative_weight * w\n",
    "        box_in_input_space = box_at_layer / np.abs(cumulative_weight_after)\n",
    "        \n",
    "        boxes.append({\n",
    "            'layer': i + 1,\n",
    "            'box_half_widths_output_space': box_at_layer.copy(),\n",
    "            'box_half_widths_input_space': box_in_input_space.copy(),\n",
    "            'value_at_layer_input': val.copy(),\n",
    "            'weight_this_layer': w.copy(),\n",
    "            'cumulative_weight': cumulative_weight_after.copy()\n",
    "        })\n",
    "        \n",
    "        # Update for next layer\n",
    "        val = w * val\n",
    "        cumulative_weight = cumulative_weight_after\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "\n",
    "boxes = compute_error_boxes_through_network(x_input, quant_weights, delta)\n",
    "\n",
    "print(\"\\n\\nError boxes per layer (mapped to input space):\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Layer':<7} {'Half-widths (input space)':<45} {'Weight this layer':<25}\")\n",
    "print(\"-\" * 90)\n",
    "for b in boxes:\n",
    "    hw = b['box_half_widths_input_space']\n",
    "    w = b['weight_this_layer']\n",
    "    print(f\"{b['layer']:<7} [{hw[0]:.6f}, {hw[1]:.6f}, {hw[2]:.6f}]   [{w[0]:.3f}, {w[1]:.3f}, {w[2]:.3f}]\")\n",
    "\n",
    "# Minkowski sum\n",
    "total_box_half_widths = np.zeros(3)\n",
    "for b in boxes:\n",
    "    total_box_half_widths += b['box_half_widths_input_space']\n",
    "\n",
    "print(f\"\\nTotal error box (Minkowski sum):\")\n",
    "print(f\"  Half-widths: [{total_box_half_widths[0]:.6f}, {total_box_half_widths[1]:.6f}, {total_box_half_widths[2]:.6f}]\")\n",
    "print(f\"\\nRelative error per channel:\")\n",
    "for i in range(3):\n",
    "    rel_err = total_box_half_widths[i] / x_input[i]\n",
    "    print(f\"  Channel {i} (input={x_input[i]:.0f}): ±{100*rel_err:.4f}%\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "\n",
    "def draw_box_3d(ax, center, half_widths, color, alpha=0.3, label=None):\n",
    "    \"\"\"Draw a 3D box.\"\"\"\n",
    "    hw = np.array(half_widths)\n",
    "    vertices = np.array(list(product([-1, 1], repeat=3))) * hw + center\n",
    "    \n",
    "    faces = [\n",
    "        [vertices[0], vertices[1], vertices[3], vertices[2]],\n",
    "        [vertices[4], vertices[5], vertices[7], vertices[6]],\n",
    "        [vertices[0], vertices[1], vertices[5], vertices[4]],\n",
    "        [vertices[2], vertices[3], vertices[7], vertices[6]],\n",
    "        [vertices[0], vertices[2], vertices[6], vertices[4]],\n",
    "        [vertices[1], vertices[3], vertices[7], vertices[5]],\n",
    "    ]\n",
    "    \n",
    "    ax.add_collection3d(Poly3DCollection(\n",
    "        faces, alpha=alpha, facecolor=color, edgecolor='black', linewidth=0.5\n",
    "    ))\n",
    "\n",
    "\n",
    "# Figure 1: Compare uniform vs non-uniform weights\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Recompute uniform case for comparison\n",
    "uniform_weights = np.ones_like(true_weights) * true_weights.mean()\n",
    "uniform_quant = quantize(uniform_weights, delta)\n",
    "boxes_uniform = compute_error_boxes_through_network(x_input, uniform_quant, delta)\n",
    "total_uniform = sum(b['box_half_widths_input_space'] for b in boxes_uniform)\n",
    "\n",
    "# Plot 1: Non-uniform error box\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "draw_box_3d(ax1, np.zeros(3), total_box_half_widths, 'red', alpha=0.4)\n",
    "ax1.set_xlabel('Ch0 (x=10)')\n",
    "ax1.set_ylabel('Ch1 (x=20)')\n",
    "ax1.set_zlabel('Ch2 (x=30)')\n",
    "ax1.set_title('Non-uniform weights\\nError box')\n",
    "# Set equal aspect ratio approximately\n",
    "max_hw = max(total_box_half_widths)\n",
    "ax1.set_xlim(-max_hw*1.2, max_hw*1.2)\n",
    "ax1.set_ylim(-max_hw*1.2, max_hw*1.2)\n",
    "ax1.set_zlim(-max_hw*1.2, max_hw*1.2)\n",
    "\n",
    "# Plot 2: Uniform error box (for comparison)\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "draw_box_3d(ax2, np.zeros(3), total_uniform, 'blue', alpha=0.4)\n",
    "ax2.set_xlabel('Ch0 (x=10)')\n",
    "ax2.set_ylabel('Ch1 (x=20)')\n",
    "ax2.set_zlabel('Ch2 (x=30)')\n",
    "ax2.set_title('Uniform weights (same mean)\\nError box')\n",
    "ax2.set_xlim(-max_hw*1.2, max_hw*1.2)\n",
    "ax2.set_ylim(-max_hw*1.2, max_hw*1.2)\n",
    "ax2.set_zlim(-max_hw*1.2, max_hw*1.2)\n",
    "\n",
    "# Plot 3: Overlay both\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "draw_box_3d(ax3, np.zeros(3), total_box_half_widths, 'red', alpha=0.3)\n",
    "draw_box_3d(ax3, np.zeros(3), total_uniform, 'blue', alpha=0.3)\n",
    "ax3.set_xlabel('Ch0 (x=10)')\n",
    "ax3.set_ylabel('Ch1 (x=20)')\n",
    "ax3.set_zlabel('Ch2 (x=30)')\n",
    "ax3.set_title('Overlay\\nRed=Non-uniform, Blue=Uniform')\n",
    "ax3.set_xlim(-max_hw*1.2, max_hw*1.2)\n",
    "ax3.set_ylim(-max_hw*1.2, max_hw*1.2)\n",
    "ax3.set_zlim(-max_hw*1.2, max_hw*1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment2_comparison_boxes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: Per-layer and per-channel analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Row 1: Error analysis\n",
    "# Plot 1: Per-layer contribution per channel\n",
    "ax = axes[0, 0]\n",
    "layer_nums = [b['layer'] for b in boxes]\n",
    "width = 0.25\n",
    "for ch in range(3):\n",
    "    contributions = [b['box_half_widths_input_space'][ch] for b in boxes]\n",
    "    ax.bar(np.array(layer_nums) + (ch - 1) * width, contributions, width=width,\n",
    "           label=f'Ch{ch} (x={x_input[ch]:.0f})')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Error half-width (input space)')\n",
    "ax.set_title('Error contribution per layer\\n(Non-uniform weights)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative error per channel\n",
    "ax = axes[0, 1]\n",
    "for ch in range(3):\n",
    "    cumulative = np.cumsum([b['box_half_widths_input_space'][ch] for b in boxes])\n",
    "    ax.plot(layer_nums, cumulative, 'o-', linewidth=2, markersize=8,\n",
    "            label=f'Ch{ch} (x={x_input[ch]:.0f})')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Cumulative error half-width')\n",
    "ax.set_title('Cumulative error\\n(Minkowski sum)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Relative error per channel - THE KEY DIFFERENCE\n",
    "ax = axes[0, 2]\n",
    "for ch in range(3):\n",
    "    cumulative = np.cumsum([b['box_half_widths_input_space'][ch] for b in boxes])\n",
    "    percentage = 100 * cumulative / x_input[ch]\n",
    "    ax.plot(layer_nums, percentage, 'o-', linewidth=2, markersize=8,\n",
    "            label=f'Ch{ch}')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Error as % of input value')\n",
    "ax.set_title('Relative error per channel\\n(NOW DIFFERENT for each channel!)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Weight analysis\n",
    "# Plot 4: Weight values per layer\n",
    "ax = axes[1, 0]\n",
    "for ch in range(3):\n",
    "    weights = quant_weights[:, ch]\n",
    "    ax.plot(range(1, n_layers+1), weights, 'o-', linewidth=2, markersize=8,\n",
    "            label=f'Ch{ch}')\n",
    "ax.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Weight value')\n",
    "ax.set_title('Weight per channel per layer\\n(>1 amplifies, <1 shrinks)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Cumulative weight product\n",
    "ax = axes[1, 1]\n",
    "for ch in range(3):\n",
    "    cum_prod = np.cumprod(quant_weights[:, ch])\n",
    "    ax.plot(range(1, n_layers+1), cum_prod, 'o-', linewidth=2, markersize=8,\n",
    "            label=f'Ch{ch}')\n",
    "ax.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Cumulative weight product')\n",
    "ax.set_title('Total amplification per channel\\n(Product of all weights)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final comparison - relative error vs cumulative weight\n",
    "ax = axes[1, 2]\n",
    "final_rel_errors = total_box_half_widths / x_input * 100\n",
    "final_cum_weights = np.prod(quant_weights, axis=0)\n",
    "\n",
    "x_pos = np.arange(3)\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x_pos - width/2, final_rel_errors, width, label='Relative error (%)', color='red', alpha=0.7)\n",
    "ax2 = ax.twinx()\n",
    "bars2 = ax2.bar(x_pos + width/2, final_cum_weights, width, label='Cumulative weight', color='blue', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('Relative error (%)', color='red')\n",
    "ax2.set_ylabel('Cumulative weight product', color='blue')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Ch0 (x=10)', 'Ch1 (x=20)', 'Ch2 (x=30)'])\n",
    "ax.set_title('Final relative error vs weight product\\n(Higher weight product → lower relative error)')\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment2_detailed_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 3: Visualize how the box shape differs from input ratios\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Normalize by input to see the \"distortion\" from uniform\n",
    "ax = axes[0]\n",
    "uniform_relative = total_uniform / x_input\n",
    "nonuniform_relative = total_box_half_widths / x_input\n",
    "\n",
    "x_pos = np.arange(3)\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, uniform_relative * 100, width, label='Uniform weights', color='blue', alpha=0.7)\n",
    "ax.bar(x_pos + width/2, nonuniform_relative * 100, width, label='Non-uniform weights', color='red', alpha=0.7)\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('Relative error (%)')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Ch0 (x=10)', 'Ch1 (x=20)', 'Ch2 (x=30)'])\n",
    "ax.set_title('Relative error comparison\\nUniform: all same | Non-uniform: different per channel')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Show the \"error sensitivity\" - which channels are most affected\n",
    "ax = axes[1]\n",
    "# Sensitivity = relative_error / mean_relative_error\n",
    "mean_rel = nonuniform_relative.mean()\n",
    "sensitivity = nonuniform_relative / mean_rel\n",
    "\n",
    "ax.bar(x_pos, sensitivity, color=['green' if s < 1 else 'red' for s in sensitivity], alpha=0.7)\n",
    "ax.axhline(1.0, color='gray', linestyle='--', linewidth=2, label='Average sensitivity')\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('Error sensitivity (relative to mean)')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Ch0 (x=10)', 'Ch1 (x=20)', 'Ch2 (x=30)'])\n",
    "ax.set_title('Channel error sensitivity\\nGreen = below average, Red = above average')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, s in enumerate(sensitivity):\n",
    "    ax.annotate(f'{s:.2f}x', (i, s + 0.05), ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment2_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Key observations\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS - EXPERIMENT 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. RELATIVE ERROR NOW VARIES BY CHANNEL\n",
    "   - Channel 0: {100*total_box_half_widths[0]/x_input[0]:.4f}%\n",
    "   - Channel 1: {100*total_box_half_widths[1]/x_input[1]:.4f}%\n",
    "   - Channel 2: {100*total_box_half_widths[2]/x_input[2]:.4f}%\n",
    "   \n",
    "   Unlike Experiment 1, these are NOT the same!\n",
    "\n",
    "2. CUMULATIVE WEIGHT PRODUCT MATTERS\n",
    "   - Channel with highest weight product: {np.argmax(final_cum_weights)} (product={final_cum_weights.max():.4f})\n",
    "   - Channel with lowest weight product: {np.argmin(final_cum_weights)} (product={final_cum_weights.min():.4f})\n",
    "   \n",
    "   Higher cumulative weight = smaller relative error (error gets \"diluted\" by amplification)\n",
    "\n",
    "3. THE ERROR BOX IS NO LONGER PROPORTIONAL TO INPUT\n",
    "   - In Exp 1: error box ~ [1, 2, 3] (proportional to input [10, 20, 30])\n",
    "   - In Exp 2: error box is distorted by the non-uniform weights\n",
    "   - Some channels accumulate more error than their input magnitude would suggest\n",
    "\n",
    "4. ERROR SENSITIVITY IDENTIFIES VULNERABLE CHANNELS\n",
    "   - Sensitivity > 1: channel accumulates MORE error than average\n",
    "   - Sensitivity < 1: channel accumulates LESS error than average\n",
    "   - This is determined by the weight structure, not just input magnitude\n",
    "\n",
    "5. IMPLICATIONS FOR QUANTIZATION\n",
    "   - Non-uniform weights create non-uniform error sensitivity\n",
    "   - Could potentially allocate more bits to sensitive channels\n",
    "   - Or design correction layers that focus on high-sensitivity channels\n",
    "\"\"\")\n",
    "\n",
    "# Verify the relationship between cumulative weight and relative error\n",
    "print(\"\\nVerification - inverse relationship between weight product and relative error:\")\n",
    "print(\"-\" * 60)\n",
    "for ch in range(3):\n",
    "    rel_err = total_box_half_widths[ch] / x_input[ch]\n",
    "    cum_weight = final_cum_weights[ch]\n",
    "    print(f\"  Channel {ch}: rel_error={rel_err:.6f}, cum_weight={cum_weight:.4f}, product={rel_err * cum_weight:.6f}\")\n",
    "\n",
    "print(\"\\n  (The product rel_error × cum_weight should be similar across channels)\")\n",
    "print(\"  (It represents the 'raw' error before being diluted by weight amplification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Non-Diagonal Weights\n",
    "# ==================================\n",
    "# \n",
    "# Setup:\n",
    "# - Same 3×1 input \"image\": [10, 20, 30]\n",
    "# - Each layer is a FULL matrix (off-diagonal entries)\n",
    "# - Channels now MIX - error in one channel affects others\n",
    "# - The error box becomes a PARALLELEPIPED, not an axis-aligned box\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection\n",
    "from scipy.spatial import ConvexHull\n",
    "from itertools import product\n",
    "\n",
    "# ============================================================\n",
    "# Setup\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input\n",
    "x_input = np.array([10.0, 20.0, 30.0])\n",
    "\n",
    "# Quantization parameters\n",
    "bits = 8\n",
    "delta = 1.0 / (2 ** (bits - 1))\n",
    "\n",
    "# Network: 4 layers, each with FULL 3x3 matrices\n",
    "# Design matrices with different characteristics\n",
    "true_weights = [\n",
    "    # Layer 1: Mostly diagonal with small off-diagonal (mild mixing)\n",
    "    np.array([\n",
    "        [0.9, 0.1, 0.0],\n",
    "        [0.1, 1.1, 0.1],\n",
    "        [0.0, 0.1, 0.8]\n",
    "    ]),\n",
    "    # Layer 2: Rotation-like (strong mixing)\n",
    "    np.array([\n",
    "        [0.8, -0.3, 0.1],\n",
    "        [0.3, 0.8, -0.2],\n",
    "        [-0.1, 0.2, 0.9]\n",
    "    ]),\n",
    "    # Layer 3: Scaling with shear\n",
    "    np.array([\n",
    "        [1.1, 0.2, 0.0],\n",
    "        [0.0, 0.9, 0.2],\n",
    "        [0.1, 0.0, 1.0]\n",
    "    ]),\n",
    "    # Layer 4: Another rotation-like\n",
    "    np.array([\n",
    "        [0.9, 0.2, -0.1],\n",
    "        [-0.2, 1.0, 0.1],\n",
    "        [0.1, -0.1, 0.85]\n",
    "    ]),\n",
    "]\n",
    "\n",
    "n_layers = len(true_weights)\n",
    "\n",
    "def quantize(W, delta):\n",
    "    \"\"\"Quantize a matrix to nearest grid points\"\"\"\n",
    "    return np.round(W / delta) * delta\n",
    "\n",
    "# Quantize all weights\n",
    "quant_weights = [quantize(W, delta) for W in true_weights]\n",
    "weight_errors = [Wq - Wt for Wq, Wt in zip(quant_weights, true_weights)]\n",
    "\n",
    "print(\"Layer configurations:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (Wt, Wq) in enumerate(zip(true_weights, quant_weights)):\n",
    "    print(f\"\\nLayer {i+1}:\")\n",
    "    print(f\"  True weights:\\n{Wt}\")\n",
    "    print(f\"  Quantized weights:\\n{Wq}\")\n",
    "    print(f\"  Frobenius norm of error: {np.linalg.norm(Wq - Wt):.6f}\")\n",
    "    print(f\"  Spectral norm (max amplification): {np.linalg.norm(Wq, ord=2):.4f}\")\n",
    "    print(f\"  Determinant (volume scaling): {np.linalg.det(Wq):.4f}\")\n",
    "\n",
    "# Compute cumulative weight product\n",
    "cumulative_product = np.eye(3)\n",
    "print(\"\\n\\nCumulative transformation properties:\")\n",
    "print(\"-\" * 60)\n",
    "for i, W in enumerate(quant_weights):\n",
    "    cumulative_product = W @ cumulative_product\n",
    "    U, S, Vt = np.linalg.svd(cumulative_product)\n",
    "    print(f\"After layer {i+1}:\")\n",
    "    print(f\"  Singular values: [{S[0]:.4f}, {S[1]:.4f}, {S[2]:.4f}]\")\n",
    "    print(f\"  Condition number: {S.max()/S.min():.4f}\")\n",
    "    print(f\"  Determinant: {np.linalg.det(cumulative_product):.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core computation: error regions as parallelepipeds\n",
    "# ============================================================\n",
    "\n",
    "def get_hypercube_vertices(half_width, dims=3):\n",
    "    \"\"\"Get vertices of a hypercube centered at origin\"\"\"\n",
    "    return np.array(list(product([-1, 1], repeat=dims))) * half_width\n",
    "\n",
    "\n",
    "def transform_vertices(vertices, W):\n",
    "    \"\"\"Apply linear transformation to vertices\"\"\"\n",
    "    return vertices @ W.T\n",
    "\n",
    "\n",
    "def compute_error_region_vertices(x_at_layer, W, delta):\n",
    "    \"\"\"\n",
    "    Compute vertices of the error region from quantizing weights W.\n",
    "    \n",
    "    Each weight W[i,j] has error in [-delta/2, delta/2].\n",
    "    Output error = W_error @ x\n",
    "    \n",
    "    For a 3x3 matrix, W_error has 9 independent error terms.\n",
    "    The output error region is the image of a 9D hypercube under\n",
    "    the linear map that takes W_error to W_error @ x.\n",
    "    \n",
    "    But we can simplify: the output is 3D, and each output dimension\n",
    "    is a linear combination of the input weighted by error terms.\n",
    "    \n",
    "    output_error[i] = sum_j W_error[i,j] * x[j]\n",
    "                    = sum_j (in [-delta/2, delta/2]) * x[j]\n",
    "    \n",
    "    For each output dimension i, the error is in \n",
    "    [-delta/2 * sum|x|, delta/2 * sum|x|]... no wait, that's not right either.\n",
    "    \n",
    "    Actually: each output dimension i has error from 3 independent terms:\n",
    "    output_error[i] = W_err[i,0]*x[0] + W_err[i,1]*x[1] + W_err[i,2]*x[2]\n",
    "    \n",
    "    Each W_err[i,j] is independent, in [-delta/2, delta/2].\n",
    "    So output_error[i] is in a range determined by the Minkowski sum\n",
    "    of intervals [-delta/2 * |x[j]|, delta/2 * |x[j]|] for j=0,1,2.\n",
    "    \n",
    "    For axis-aligned, this would be: [-delta/2 * (|x[0]|+|x[1]|+|x[2]|), ...] per dimension.\n",
    "    But the three output dimensions are INDEPENDENT (different rows of W).\n",
    "    \n",
    "    So the error region is actually an axis-aligned box with half-widths:\n",
    "    hw[i] = delta/2 * sum_j |x[j]|\n",
    "    \n",
    "    Wait, that makes all dimensions the same. Let me reconsider...\n",
    "    \n",
    "    Actually each dimension i has:\n",
    "    hw[i] = delta/2 * sum_j |x[j]|   (since each W_err[i,j] is independent)\n",
    "    \n",
    "    So the error from weight quantization at a single layer IS an axis-aligned box,\n",
    "    but with equal half-widths (proportional to L1 norm of input).\n",
    "    \n",
    "    The non-axis-aligned shape comes from TRANSFORMING this box through subsequent layers.\n",
    "    \"\"\"\n",
    "    # Error from weight quantization: axis-aligned box\n",
    "    # Each output dim i: error = sum_j W_err[i,j] * x[j]\n",
    "    # Max error per output dim = delta/2 * sum|x|\n",
    "    l1_norm = np.sum(np.abs(x_at_layer))\n",
    "    hw = delta / 2 * l1_norm * np.ones(3)\n",
    "    \n",
    "    # Vertices of this box\n",
    "    vertices = get_hypercube_vertices(1.0, dims=3) * hw\n",
    "    \n",
    "    return vertices, hw\n",
    "\n",
    "\n",
    "def trace_error_geometry(x, quant_weights, delta):\n",
    "    \"\"\"\n",
    "    Trace the error region geometry through layers.\n",
    "    \n",
    "    Key insight: at each layer, we add a new error box (from that layer's \n",
    "    weight quantization), then transform the cumulative error by the next layer's weights.\n",
    "    \n",
    "    Returns history of error region vertices at each layer.\n",
    "    \"\"\"\n",
    "    history = []\n",
    "    \n",
    "    val = x.copy()\n",
    "    cumulative_transform = np.eye(3)  # Maps current error space back to input space\n",
    "    \n",
    "    # Track error region vertices (in input space)\n",
    "    all_error_vertices = []  # List of vertex sets, one per layer\n",
    "    \n",
    "    for i, W in enumerate(quant_weights):\n",
    "        # Error introduced at this layer (in this layer's output space)\n",
    "        error_vertices_local, hw_local = compute_error_region_vertices(val, W, delta)\n",
    "        \n",
    "        # Transform to input space using inverse of cumulative transform so far\n",
    "        # After this layer, cumulative transform becomes W @ cumulative_transform\n",
    "        cumulative_transform_after = W @ cumulative_transform\n",
    "        \n",
    "        # Map local error to input space\n",
    "        # Local error is in output space of this layer\n",
    "        # To map to input space: multiply by inverse of cumulative_transform_after\n",
    "        try:\n",
    "            inv_transform = np.linalg.inv(cumulative_transform_after)\n",
    "            error_vertices_input_space = transform_vertices(error_vertices_local, inv_transform)\n",
    "        except np.linalg.LinAlgError:\n",
    "            error_vertices_input_space = error_vertices_local  # Fallback\n",
    "        \n",
    "        all_error_vertices.append(error_vertices_input_space)\n",
    "        \n",
    "        # Compute Minkowski sum of all error regions so far\n",
    "        if i == 0:\n",
    "            total_vertices = error_vertices_input_space\n",
    "        else:\n",
    "            # Minkowski sum: all pairwise sums of vertices\n",
    "            total_vertices = minkowski_sum_vertices(total_vertices, error_vertices_input_space)\n",
    "        \n",
    "        history.append({\n",
    "            'layer': i + 1,\n",
    "            'value': val.copy(),\n",
    "            'W': W.copy(),\n",
    "            'error_vertices_local': error_vertices_local.copy(),\n",
    "            'error_vertices_input_space': error_vertices_input_space.copy(),\n",
    "            'cumulative_error_vertices': total_vertices.copy(),\n",
    "            'cumulative_transform': cumulative_transform_after.copy(),\n",
    "            'hw_local': hw_local.copy()\n",
    "        })\n",
    "        \n",
    "        # Update for next layer\n",
    "        val = W @ val\n",
    "        cumulative_transform = cumulative_transform_after\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def minkowski_sum_vertices(V1, V2):\n",
    "    \"\"\"\n",
    "    Compute Minkowski sum of two vertex sets.\n",
    "    Result vertices are all pairwise sums.\n",
    "    Then take convex hull to get the actual vertices.\n",
    "    \"\"\"\n",
    "    sums = []\n",
    "    for v1 in V1:\n",
    "        for v2 in V2:\n",
    "            sums.append(v1 + v2)\n",
    "    sums = np.array(sums)\n",
    "    \n",
    "    # Take convex hull to reduce to actual vertices\n",
    "    if len(sums) > 4:  # Need at least 4 points for 3D hull\n",
    "        try:\n",
    "            hull = ConvexHull(sums)\n",
    "            return sums[hull.vertices]\n",
    "        except:\n",
    "            return sums\n",
    "    return sums\n",
    "\n",
    "\n",
    "history = trace_error_geometry(x_input, quant_weights, delta)\n",
    "\n",
    "print(\"\\n\\nError geometry through layers:\")\n",
    "print(\"=\" * 70)\n",
    "for h in history:\n",
    "    print(f\"\\nLayer {h['layer']}:\")\n",
    "    print(f\"  Value at layer input: [{h['value'][0]:.3f}, {h['value'][1]:.3f}, {h['value'][2]:.3f}]\")\n",
    "    print(f\"  Local error box half-widths: {h['hw_local'][0]:.6f} (same for all dims)\")\n",
    "    print(f\"  Error vertices in input space: {len(h['error_vertices_input_space'])} vertices\")\n",
    "    print(f\"  Cumulative error vertices: {len(h['cumulative_error_vertices'])} vertices\")\n",
    "    \n",
    "    # Compute bounding box of cumulative error\n",
    "    cum_verts = h['cumulative_error_vertices']\n",
    "    bbox_min = cum_verts.min(axis=0)\n",
    "    bbox_max = cum_verts.max(axis=0)\n",
    "    print(f\"  Cumulative error bounding box:\")\n",
    "    print(f\"    Ch0: [{bbox_min[0]:.6f}, {bbox_max[0]:.6f}]\")\n",
    "    print(f\"    Ch1: [{bbox_min[1]:.6f}, {bbox_max[1]:.6f}]\")\n",
    "    print(f\"    Ch2: [{bbox_min[2]:.6f}, {bbox_max[2]:.6f}]\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "\n",
    "def draw_vertices_and_hull(ax, vertices, color, alpha=0.3, label=None):\n",
    "    \"\"\"Draw vertices and their convex hull\"\"\"\n",
    "    ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], \n",
    "               c=color, s=20, alpha=0.8)\n",
    "    \n",
    "    if len(vertices) >= 4:\n",
    "        try:\n",
    "            hull = ConvexHull(vertices)\n",
    "            for simplex in hull.simplices:\n",
    "                triangle = vertices[simplex]\n",
    "                ax.add_collection3d(Poly3DCollection(\n",
    "                    [triangle], alpha=alpha, facecolor=color, edgecolor='black', linewidth=0.5\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def draw_wireframe_box(ax, half_widths, color='gray', alpha=0.5):\n",
    "    \"\"\"Draw wireframe of axis-aligned box for reference\"\"\"\n",
    "    hw = half_widths\n",
    "    # 12 edges of a box\n",
    "    edges = []\n",
    "    for i in [-1, 1]:\n",
    "        for j in [-1, 1]:\n",
    "            edges.append([[-hw[0]*i, -hw[1]*j, -hw[2]], [-hw[0]*i, -hw[1]*j, hw[2]]])\n",
    "            edges.append([[-hw[0]*i, -hw[1], -hw[2]*j], [-hw[0]*i, hw[1], -hw[2]*j]])\n",
    "            edges.append([[-hw[0], -hw[1]*i, -hw[2]*j], [hw[0], -hw[1]*i, -hw[2]*j]])\n",
    "    \n",
    "    for edge in edges:\n",
    "        ax.plot3D(*zip(*edge), color=color, alpha=alpha, linewidth=1)\n",
    "\n",
    "\n",
    "# Figure 1: Error region evolution through layers\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, n_layers))\n",
    "\n",
    "for i, (h, color) in enumerate(zip(history, colors)):\n",
    "    ax = fig.add_subplot(1, 4, i+1, projection='3d')\n",
    "    \n",
    "    # Draw this layer's error contribution (in input space)\n",
    "    draw_vertices_and_hull(ax, h['error_vertices_input_space'], color, alpha=0.4)\n",
    "    \n",
    "    # Draw cumulative error region\n",
    "    draw_vertices_and_hull(ax, h['cumulative_error_vertices'], 'red', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Ch0')\n",
    "    ax.set_ylabel('Ch1')\n",
    "    ax.set_zlabel('Ch2')\n",
    "    ax.set_title(f\"Layer {h['layer']}\\n{len(h['cumulative_error_vertices'])} vertices\")\n",
    "    \n",
    "    # Set consistent scale\n",
    "    max_extent = np.abs(h['cumulative_error_vertices']).max() * 1.2\n",
    "    ax.set_xlim(-max_extent, max_extent)\n",
    "    ax.set_ylim(-max_extent, max_extent)\n",
    "    ax.set_zlim(-max_extent, max_extent)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment3_error_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: Compare final error region to axis-aligned box\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "final_vertices = history[-1]['cumulative_error_vertices']\n",
    "bbox_hw = (final_vertices.max(axis=0) - final_vertices.min(axis=0)) / 2\n",
    "\n",
    "# Plot 1: Final error region (the actual parallelepiped-ish shape)\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "draw_vertices_and_hull(ax1, final_vertices, 'red', alpha=0.4)\n",
    "ax1.set_xlabel('Ch0')\n",
    "ax1.set_ylabel('Ch1')\n",
    "ax1.set_zlabel('Ch2')\n",
    "ax1.set_title('Actual error region\\n(Non-axis-aligned)')\n",
    "max_extent = np.abs(final_vertices).max() * 1.2\n",
    "ax1.set_xlim(-max_extent, max_extent)\n",
    "ax1.set_ylim(-max_extent, max_extent)\n",
    "ax1.set_zlim(-max_extent, max_extent)\n",
    "\n",
    "# Plot 2: Bounding box (axis-aligned approximation)\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "box_vertices = get_hypercube_vertices(1.0) * bbox_hw\n",
    "draw_vertices_and_hull(ax2, box_vertices, 'blue', alpha=0.4)\n",
    "ax2.set_xlabel('Ch0')\n",
    "ax2.set_ylabel('Ch1')\n",
    "ax2.set_zlabel('Ch2')\n",
    "ax2.set_title('Bounding box\\n(Axis-aligned approximation)')\n",
    "ax2.set_xlim(-max_extent, max_extent)\n",
    "ax2.set_ylim(-max_extent, max_extent)\n",
    "ax2.set_zlim(-max_extent, max_extent)\n",
    "\n",
    "# Plot 3: Overlay\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "draw_vertices_and_hull(ax3, final_vertices, 'red', alpha=0.3)\n",
    "draw_wireframe_box(ax3, bbox_hw, 'blue', alpha=0.8)\n",
    "ax3.set_xlabel('Ch0')\n",
    "ax3.set_ylabel('Ch1')\n",
    "ax3.set_zlabel('Ch2')\n",
    "ax3.set_title('Overlay\\nRed=Actual, Blue wireframe=Bounding box')\n",
    "ax3.set_xlim(-max_extent, max_extent)\n",
    "ax3.set_ylim(-max_extent, max_extent)\n",
    "ax3.set_zlim(-max_extent, max_extent)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment3_final_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 3: Analyze the shape via PCA / SVD\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Compute SVD of the final error region\n",
    "centered = final_vertices - final_vertices.mean(axis=0)\n",
    "U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "\n",
    "print(\"\\n\\nSVD analysis of final error region:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Singular values: [{S[0]:.6f}, {S[1]:.6f}, {S[2]:.6f}]\")\n",
    "print(f\"Condition number: {S[0]/S[2]:.4f}\")\n",
    "print(f\"Principal directions:\")\n",
    "for i, v in enumerate(Vt):\n",
    "    print(f\"  PC{i+1}: [{v[0]:.4f}, {v[1]:.4f}, {v[2]:.4f}]\")\n",
    "\n",
    "# Plot 1: Singular values (shape elongation)\n",
    "ax = axes[0, 0]\n",
    "ax.bar(range(3), S, color=['red', 'green', 'blue'], alpha=0.7)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels(['PC1', 'PC2', 'PC3'])\n",
    "ax.set_ylabel('Singular value')\n",
    "ax.set_title('Error region principal components\\n(Higher = more spread in that direction)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Compare to diagonal case\n",
    "# Recompute with diagonal weights for comparison\n",
    "diagonal_weights = [np.diag(np.diag(W)) for W in quant_weights]\n",
    "history_diag = trace_error_geometry(x_input, diagonal_weights, delta)\n",
    "final_vertices_diag = history_diag[-1]['cumulative_error_vertices']\n",
    "centered_diag = final_vertices_diag - final_vertices_diag.mean(axis=0)\n",
    "U_diag, S_diag, Vt_diag = np.linalg.svd(centered_diag, full_matrices=False)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "x_pos = np.arange(3)\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, S, width, label='Full matrix', color='red', alpha=0.7)\n",
    "ax.bar(x_pos + width/2, S_diag, width, label='Diagonal only', color='blue', alpha=0.7)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['PC1', 'PC2', 'PC3'])\n",
    "ax.set_ylabel('Singular value')\n",
    "ax.set_title('Shape comparison: Full vs Diagonal weights')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Volume comparison through layers\n",
    "ax = axes[1, 0]\n",
    "volumes_full = []\n",
    "volumes_diag = []\n",
    "\n",
    "for h_full, h_diag in zip(history, history_diag):\n",
    "    # Approximate volume using convex hull\n",
    "    try:\n",
    "        hull_full = ConvexHull(h_full['cumulative_error_vertices'])\n",
    "        vol_full = hull_full.volume\n",
    "    except:\n",
    "        vol_full = 0\n",
    "    try:\n",
    "        hull_diag = ConvexHull(h_diag['cumulative_error_vertices'])\n",
    "        vol_diag = hull_diag.volume\n",
    "    except:\n",
    "        vol_diag = 0\n",
    "    volumes_full.append(vol_full)\n",
    "    volumes_diag.append(vol_diag)\n",
    "\n",
    "layers = [h['layer'] for h in history]\n",
    "ax.plot(layers, volumes_full, 'o-', linewidth=2, markersize=8, label='Full matrix', color='red')\n",
    "ax.plot(layers, volumes_diag, 's-', linewidth=2, markersize=8, label='Diagonal only', color='blue')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Error region volume')\n",
    "ax.set_title('Error volume growth\\n(Full matrices can have different volume than diagonal)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Bounding box inefficiency\n",
    "# How much of the bounding box is \"wasted\" (not part of actual error region)?\n",
    "ax = axes[1, 1]\n",
    "bbox_volumes = []\n",
    "actual_volumes = []\n",
    "\n",
    "for h in history:\n",
    "    verts = h['cumulative_error_vertices']\n",
    "    bbox = np.prod(verts.max(axis=0) - verts.min(axis=0))\n",
    "    bbox_volumes.append(bbox)\n",
    "    try:\n",
    "        hull = ConvexHull(verts)\n",
    "        actual_volumes.append(hull.volume)\n",
    "    except:\n",
    "        actual_volumes.append(bbox)\n",
    "\n",
    "efficiency = [a/b if b > 0 else 1 for a, b in zip(actual_volumes, bbox_volumes)]\n",
    "\n",
    "ax.bar(layers, efficiency, color='purple', alpha=0.7)\n",
    "ax.axhline(1.0, color='gray', linestyle='--', label='Perfect efficiency (cube)')\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Volume efficiency (actual / bounding box)')\n",
    "ax.set_title('Bounding box efficiency\\n(<1 means error region is tilted/elongated)')\n",
    "ax.set_ylim(0, 1.2)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment3_shape_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 4: 2D projections to see the tilt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "projections = [(0, 1, 'Ch0', 'Ch1'), (0, 2, 'Ch0', 'Ch2'), (1, 2, 'Ch1', 'Ch2')]\n",
    "\n",
    "for ax, (i, j, xlabel, ylabel) in zip(axes, projections):\n",
    "    # Project final vertices\n",
    "    proj_full = final_vertices[:, [i, j]]\n",
    "    proj_diag = final_vertices_diag[:, [i, j]]\n",
    "    \n",
    "    # Draw convex hulls\n",
    "    try:\n",
    "        hull_full = ConvexHull(proj_full)\n",
    "        hull_diag = ConvexHull(proj_diag)\n",
    "        \n",
    "        # Full matrix\n",
    "        for simplex in hull_full.simplices:\n",
    "            ax.plot(proj_full[simplex, 0], proj_full[simplex, 1], 'r-', linewidth=2)\n",
    "        ax.fill(proj_full[hull_full.vertices, 0], proj_full[hull_full.vertices, 1], \n",
    "                'red', alpha=0.3, label='Full matrix')\n",
    "        \n",
    "        # Diagonal\n",
    "        for simplex in hull_diag.simplices:\n",
    "            ax.plot(proj_diag[simplex, 0], proj_diag[simplex, 1], 'b--', linewidth=2)\n",
    "        ax.fill(proj_diag[hull_diag.vertices, 0], proj_diag[hull_diag.vertices, 1], \n",
    "                'blue', alpha=0.2, label='Diagonal only')\n",
    "    except:\n",
    "        ax.scatter(proj_full[:, 0], proj_full[:, 1], c='red', alpha=0.5)\n",
    "        ax.scatter(proj_diag[:, 0], proj_diag[:, 1], c='blue', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f'Projection onto {xlabel}-{ylabel} plane')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment3_projections.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Key observations\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS - EXPERIMENT 3\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. ERROR REGION IS NO LONGER AXIS-ALIGNED\n",
    "   - Off-diagonal weights cause channels to mix\n",
    "   - Error in one channel propagates to others\n",
    "   - The error region becomes a parallelepiped (or more complex polytope)\n",
    "\n",
    "2. THE SHAPE IS TILTED\n",
    "   - Principal components are not aligned with coordinate axes\n",
    "   - PC1 direction: [{Vt[0,0]:.3f}, {Vt[0,1]:.3f}, {Vt[0,2]:.3f}]\n",
    "   - This means error is correlated across channels\n",
    "\n",
    "3. BOUNDING BOX OVERESTIMATES ERROR\n",
    "   - Actual volume / Bounding box volume = {efficiency[-1]:.3f}\n",
    "   - Using axis-aligned bounds would overestimate error by {100*(1/efficiency[-1] - 1):.1f}%\n",
    "   - The true error region is more constrained than the box suggests\n",
    "\n",
    "4. SINGULAR VALUES REVEAL ERROR ANISOTROPY\n",
    "   - Largest SV: {S[0]:.6f} (most error in this direction)\n",
    "   - Smallest SV: {S[2]:.6f} (least error in this direction)\n",
    "   - Ratio: {S[0]/S[2]:.2f}x (error is {S[0]/S[2]:.1f}x larger in worst vs best direction)\n",
    "\n",
    "5. COMPARISON TO DIAGONAL CASE\n",
    "   - Full matrix volume: {volumes_full[-1]:.6f}\n",
    "   - Diagonal-only volume: {volumes_diag[-1]:.6f}\n",
    "   - Ratio: {volumes_full[-1]/volumes_diag[-1]:.3f}x\n",
    "   \n",
    "6. IMPLICATIONS\n",
    "   - Can't analyze channels independently when weights mix them\n",
    "   - Error correction needs to account for correlations\n",
    "   - The \"worst case\" error direction may not align with any single channel\n",
    "   - PCA of error region tells you where to focus correction efforts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0edd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Multiple Input Points - Error Manifold\n",
    "# ====================================================\n",
    "# \n",
    "# Setup:\n",
    "# - Multiple input points (not just one)\n",
    "# - See how error region varies across input space\n",
    "# - Connect input geometry to error geometry\n",
    "#\n",
    "# Key question: How does the \"error manifold\" relate to the \"input manifold\"?\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# ============================================================\n",
    "# Setup - Keep it simple: 2D for visualization\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Quantization\n",
    "bits = 8\n",
    "delta = 1.0 / (2 ** (bits - 1))\n",
    "\n",
    "# Network: 3 layers, 2D -> 2D\n",
    "# Mix of diagonal and off-diagonal to show both effects\n",
    "true_weights = [\n",
    "    np.array([[0.9, 0.2],\n",
    "              [0.1, 1.1]]),\n",
    "    np.array([[1.1, -0.1],\n",
    "              [0.2, 0.8]]),\n",
    "    np.array([[0.85, 0.15],\n",
    "              [-0.1, 1.0]]),\n",
    "]\n",
    "\n",
    "def quantize(W, delta):\n",
    "    return np.round(W / delta) * delta\n",
    "\n",
    "quant_weights = [quantize(W, delta) for W in true_weights]\n",
    "\n",
    "print(\"Network configuration (2D):\")\n",
    "print(\"-\" * 40)\n",
    "for i, W in enumerate(quant_weights):\n",
    "    print(f\"Layer {i+1}:\\n{W}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core: compute error for a single input point\n",
    "# ============================================================\n",
    "\n",
    "def compute_error_half_width(x, quant_weights, delta):\n",
    "    \"\"\"\n",
    "    Compute the error half-width at each layer for input x,\n",
    "    mapped back to input space.\n",
    "    \n",
    "    Returns total error half-widths (as a 2D vector for the bounding box).\n",
    "    \"\"\"\n",
    "    val = x.copy()\n",
    "    cumulative_W = np.eye(2)\n",
    "    \n",
    "    total_error_vertices = None\n",
    "    \n",
    "    for W in quant_weights:\n",
    "        # Error at this layer: each output is sum of weight_err * input\n",
    "        # For 2D: output_err[i] = sum_j W_err[i,j] * val[j]\n",
    "        # Max error per output dim = delta/2 * sum|val|\n",
    "        l1_norm = np.sum(np.abs(val))\n",
    "        local_hw = delta / 2 * l1_norm\n",
    "        \n",
    "        # This gives an axis-aligned box in output space\n",
    "        # Vertices of local error box\n",
    "        local_vertices = np.array([[-1, -1], [-1, 1], [1, 1], [1, -1]]) * local_hw\n",
    "        \n",
    "        # Map to input space via inverse of cumulative transform\n",
    "        cumulative_W = W @ cumulative_W\n",
    "        try:\n",
    "            inv_W = np.linalg.inv(cumulative_W)\n",
    "            local_vertices_input = local_vertices @ inv_W.T\n",
    "        except:\n",
    "            local_vertices_input = local_vertices\n",
    "        \n",
    "        # Minkowski sum\n",
    "        if total_error_vertices is None:\n",
    "            total_error_vertices = local_vertices_input\n",
    "        else:\n",
    "            # Pairwise sums\n",
    "            new_vertices = []\n",
    "            for v1 in total_error_vertices:\n",
    "                for v2 in local_vertices_input:\n",
    "                    new_vertices.append(v1 + v2)\n",
    "            new_vertices = np.array(new_vertices)\n",
    "            # Convex hull to simplify\n",
    "            try:\n",
    "                hull = ConvexHull(new_vertices)\n",
    "                total_error_vertices = new_vertices[hull.vertices]\n",
    "            except:\n",
    "                total_error_vertices = new_vertices\n",
    "        \n",
    "        # Update value for next layer\n",
    "        val = W @ val\n",
    "    \n",
    "    return total_error_vertices\n",
    "\n",
    "\n",
    "def compute_error_magnitude(x, quant_weights, delta):\n",
    "    \"\"\"Compute scalar error magnitude (max extent) for a point.\"\"\"\n",
    "    vertices = compute_error_half_width(x, quant_weights, delta)\n",
    "    return np.max(np.linalg.norm(vertices, axis=1))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Generate input points on different manifolds\n",
    "# ============================================================\n",
    "\n",
    "# Manifold 1: Circle\n",
    "n_points = 50\n",
    "theta = np.linspace(0, 2*np.pi, n_points, endpoint=False)\n",
    "radius = 20\n",
    "circle_points = np.column_stack([radius * np.cos(theta), radius * np.sin(theta)])\n",
    "\n",
    "# Manifold 2: Line segment\n",
    "line_points = np.column_stack([\n",
    "    np.linspace(-30, 30, n_points),\n",
    "    np.linspace(-10, 10, n_points)\n",
    "])\n",
    "\n",
    "# Manifold 3: Grid (to see error variation across 2D space)\n",
    "grid_1d = np.linspace(-30, 30, 15)\n",
    "grid_x, grid_y = np.meshgrid(grid_1d, grid_1d)\n",
    "grid_points = np.column_stack([grid_x.ravel(), grid_y.ravel()])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Compute errors for all points\n",
    "# ============================================================\n",
    "\n",
    "print(\"Computing error regions for input manifolds...\")\n",
    "\n",
    "# For circle\n",
    "circle_errors = [compute_error_magnitude(p, quant_weights, delta) for p in circle_points]\n",
    "circle_vertices = [compute_error_half_width(p, quant_weights, delta) for p in circle_points]\n",
    "\n",
    "# For line\n",
    "line_errors = [compute_error_magnitude(p, quant_weights, delta) for p in line_points]\n",
    "\n",
    "# For grid\n",
    "grid_errors = np.array([compute_error_magnitude(p, quant_weights, delta) for p in grid_points])\n",
    "grid_errors_2d = grid_errors.reshape(len(grid_1d), len(grid_1d))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Visualization\n",
    "# ============================================================\n",
    "\n",
    "# Figure 1: Error magnitude varies with input position\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Circle - colored by error magnitude\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(circle_points[:, 0], circle_points[:, 1], \n",
    "                     c=circle_errors, cmap='hot', s=100, edgecolors='black')\n",
    "plt.colorbar(scatter, ax=ax, label='Error magnitude')\n",
    "ax.set_xlabel('Input dim 0')\n",
    "ax.set_ylabel('Input dim 1')\n",
    "ax.set_title('Circle manifold\\nColor = error magnitude')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Line - error vs position\n",
    "ax = axes[1]\n",
    "positions = np.linalg.norm(line_points, axis=1) * np.sign(line_points[:, 0])\n",
    "ax.plot(positions, line_errors, 'o-', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Position along line')\n",
    "ax.set_ylabel('Error magnitude')\n",
    "ax.set_title('Line manifold\\nError grows with distance from origin')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Grid - heatmap\n",
    "ax = axes[2]\n",
    "im = ax.imshow(grid_errors_2d, extent=[-30, 30, -30, 30], origin='lower', cmap='hot')\n",
    "plt.colorbar(im, ax=ax, label='Error magnitude')\n",
    "ax.set_xlabel('Input dim 0')\n",
    "ax.set_ylabel('Input dim 1')\n",
    "ax.set_title('Error magnitude across input space\\nBrighter = more error')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment4_error_magnitude.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 2: Error regions for selected points on the circle\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Select 8 points around the circle\n",
    "selected_indices = np.linspace(0, n_points-1, 8, dtype=int)\n",
    "\n",
    "for idx, (ax, i) in enumerate(zip(axes.flat, selected_indices)):\n",
    "    point = circle_points[i]\n",
    "    vertices = circle_vertices[i]\n",
    "    \n",
    "    # Draw error region centered at origin\n",
    "    if len(vertices) >= 3:\n",
    "        hull = ConvexHull(vertices)\n",
    "        hull_vertices = vertices[hull.vertices]\n",
    "        hull_vertices = np.vstack([hull_vertices, hull_vertices[0]])  # Close polygon\n",
    "        ax.fill(hull_vertices[:, 0], hull_vertices[:, 1], 'red', alpha=0.3)\n",
    "        ax.plot(hull_vertices[:, 0], hull_vertices[:, 1], 'r-', linewidth=2)\n",
    "    \n",
    "    ax.scatter([0], [0], c='black', s=50, zorder=5)\n",
    "    \n",
    "    ax.set_title(f'Input: ({point[0]:.0f}, {point[1]:.0f})\\nError: {circle_errors[i]:.4f}')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set consistent scale\n",
    "    max_err = max(circle_errors) * 1.2\n",
    "    ax.set_xlim(-max_err, max_err)\n",
    "    ax.set_ylim(-max_err, max_err)\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('Error regions for points around the circle\\n(Shape changes with input direction!)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment4_error_regions_circle.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 3: The key insight - error magnitude vs input magnitude\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Error vs L1 norm of input\n",
    "ax = axes[0]\n",
    "l1_norms = np.sum(np.abs(grid_points), axis=1)\n",
    "ax.scatter(l1_norms, grid_errors, alpha=0.5, s=20)\n",
    "\n",
    "# Fit line\n",
    "z = np.polyfit(l1_norms, grid_errors, 1)\n",
    "p = np.poly1d(z)\n",
    "x_fit = np.linspace(l1_norms.min(), l1_norms.max(), 100)\n",
    "ax.plot(x_fit, p(x_fit), 'r-', linewidth=2, label=f'Linear fit: y = {z[0]:.4f}x + {z[1]:.4f}')\n",
    "\n",
    "ax.set_xlabel('L1 norm of input (|x₀| + |x₁|)')\n",
    "ax.set_ylabel('Error magnitude')\n",
    "ax.set_title('Error scales with input L1 norm\\n(But with scatter due to direction)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error vs L2 norm\n",
    "ax = axes[1]\n",
    "l2_norms = np.linalg.norm(grid_points, axis=1)\n",
    "ax.scatter(l2_norms, grid_errors, alpha=0.5, s=20, c=np.arctan2(grid_points[:,1], grid_points[:,0]), cmap='hsv')\n",
    "\n",
    "ax.set_xlabel('L2 norm of input (distance from origin)')\n",
    "ax.set_ylabel('Error magnitude')\n",
    "ax.set_title('Error vs distance from origin\\nColor = angle (shows directional dependence)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment4_error_vs_norm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Figure 4: Error region shape varies with direction\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Draw all error regions overlaid, centered at their input points (scaled down)\n",
    "scale = 0.3  # Scale factor to make error regions visible\n",
    "\n",
    "for i, (point, vertices) in enumerate(zip(circle_points, circle_vertices)):\n",
    "    if len(vertices) >= 3:\n",
    "        try:\n",
    "            hull = ConvexHull(vertices)\n",
    "            hull_vertices = vertices[hull.vertices] * scale + point\n",
    "            hull_vertices = np.vstack([hull_vertices, hull_vertices[0]])\n",
    "            ax.fill(hull_vertices[:, 0], hull_vertices[:, 1], 'red', alpha=0.2)\n",
    "            ax.plot(hull_vertices[:, 0], hull_vertices[:, 1], 'r-', linewidth=0.5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Draw the circle\n",
    "ax.plot(circle_points[:, 0], circle_points[:, 1], 'b-', linewidth=2, label='Input manifold (circle)')\n",
    "ax.scatter(circle_points[:, 0], circle_points[:, 1], c='blue', s=30, zorder=5)\n",
    "\n",
    "ax.set_xlabel('Input dim 0')\n",
    "ax.set_ylabel('Input dim 1')\n",
    "ax.set_title('Input manifold with error regions\\n(Red shapes show error region at each point, scaled)')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/experiment4_manifold_with_errors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Key observations\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS - EXPERIMENT 4\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compute some statistics\n",
    "error_mean = np.mean(circle_errors)\n",
    "error_std = np.std(circle_errors)\n",
    "error_min = np.min(circle_errors)\n",
    "error_max = np.max(circle_errors)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. ERROR MAGNITUDE SCALES WITH INPUT MAGNITUDE\n",
    "   - Points further from origin have larger error\n",
    "   - This is because error = weight_error × value\n",
    "   - Larger values → larger absolute error\n",
    "\n",
    "2. ERROR MAGNITUDE DEPENDS ON DIRECTION\n",
    "   - Points at same distance but different angles have different errors\n",
    "   - On the circle (constant radius), error varies from {error_min:.4f} to {error_max:.4f}\n",
    "   - Ratio: {error_max/error_min:.2f}x variation\n",
    "   - This is due to non-diagonal weights creating directional preference\n",
    "\n",
    "3. ERROR REGION SHAPE VARIES WITH INPUT\n",
    "   - Not just magnitude — the shape (orientation) changes\n",
    "   - Error region \"tilts\" based on input direction\n",
    "   - This comes from how the cumulative weight transform depends on path through network\n",
    "\n",
    "4. THE ERROR MANIFOLD\n",
    "   - Input manifold: circle of radius {radius}\n",
    "   - Error at each point creates a \"tube\" around the manifold\n",
    "   - The tube thickness varies with position\n",
    "   - The tube cross-section shape varies with position\n",
    "\n",
    "5. IMPLICATIONS FOR QUANTIZATION\n",
    "   - Some directions in input space are \"safer\" than others\n",
    "   - Could potentially transform data to align with low-error directions\n",
    "   - For classification: if decision boundary aligns with high-error direction, more mistakes\n",
    "\n",
    "6. RELATIONSHIP: ERROR ~ k × L1_NORM (approximately)\n",
    "   - Linear fit slope: {z[0]:.4f}\n",
    "   - But there's scatter due to directional effects\n",
    "   - L1 norm matters because error = delta × sum(|weights| × |inputs|)\n",
    "\"\"\")\n",
    "\n",
    "# Verify the L1 relationship\n",
    "print(\"\\nVerification - error vs L1 norm for circle points:\")\n",
    "circle_l1 = np.sum(np.abs(circle_points), axis=1)\n",
    "correlation = np.corrcoef(circle_l1, circle_errors)[0, 1]\n",
    "print(f\"  Correlation between L1 norm and error: {correlation:.4f}\")\n",
    "print(f\"  (Not perfect because direction also matters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-aware-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
