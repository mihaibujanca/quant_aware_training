{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080be560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class ErrorDecomposition:\n",
    "    \"\"\"Track different sources of error at each layer\"\"\"\n",
    "    layer_idx: int\n",
    "    \n",
    "    # Linear (compensatable) errors\n",
    "    weight_quant_error: np.ndarray      # (W_q - W) @ x\n",
    "    activation_round_error: np.ndarray   # Rounding without saturation\n",
    "    \n",
    "    # Nonlinear (non-compensatable) errors  \n",
    "    saturation_error: np.ndarray         # From clipping to [qmin, qmax]\n",
    "    relu_interaction_error: np.ndarray   # ReLU(x + e) - ReLU(x) - linear_approx\n",
    "    \n",
    "    # Metadata\n",
    "    n_saturated: int                     # Count of saturated activations\n",
    "    n_relu_flipped: int                  # Count where ReLU decision flipped\n",
    "    \n",
    "    @property\n",
    "    def total_linear_error(self):\n",
    "        return self.weight_quant_error + self.activation_round_error\n",
    "    \n",
    "    @property\n",
    "    def total_nonlinear_error(self):\n",
    "        return self.saturation_error + self.relu_interaction_error\n",
    "    \n",
    "    @property\n",
    "    def total_error(self):\n",
    "        return self.total_linear_error + self.total_nonlinear_error\n",
    "    \n",
    "    @property\n",
    "    def linear_fraction(self):\n",
    "        \"\"\"What fraction of error is linearly compensatable?\"\"\"\n",
    "        lin_norm = np.linalg.norm(self.total_linear_error)\n",
    "        total_norm = np.linalg.norm(self.total_error)\n",
    "        return lin_norm / total_norm if total_norm > 0 else 1.0\n",
    "    \n",
    "    def summary(self):\n",
    "        return {\n",
    "            'layer': self.layer_idx,\n",
    "            'weight_quant': np.linalg.norm(self.weight_quant_error),\n",
    "            'round': np.linalg.norm(self.activation_round_error),\n",
    "            'saturation': np.linalg.norm(self.saturation_error),\n",
    "            'relu_flip': np.linalg.norm(self.relu_interaction_error),\n",
    "            'n_saturated': self.n_saturated,\n",
    "            'n_relu_flipped': self.n_relu_flipped,\n",
    "            'linear_fraction': self.linear_fraction,\n",
    "        }\n",
    "\n",
    "\n",
    "class ErrorTracker:\n",
    "    \"\"\"\n",
    "    Run parallel FP32 and quantized forward passes,\n",
    "    decomposing error at each layer into linear vs nonlinear components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bits=8, symmetric=True):\n",
    "        self.bits = bits\n",
    "        self.symmetric = symmetric\n",
    "        if symmetric:\n",
    "            self.qmin = -(2**(bits-1))\n",
    "            self.qmax = 2**(bits-1) - 1\n",
    "        else:\n",
    "            self.qmin = 0\n",
    "            self.qmax = 2**bits - 1\n",
    "    \n",
    "    def quantize(self, x, scale=None):\n",
    "        \"\"\"Quantize and return (quantized, scale, was_saturated_mask)\"\"\"\n",
    "        x_np = x.numpy() if isinstance(x, torch.Tensor) else x\n",
    "        \n",
    "        if scale is None:\n",
    "            if self.symmetric:\n",
    "                scale = np.abs(x_np).max() / self.qmax if np.abs(x_np).max() > 0 else 1.0\n",
    "            else:\n",
    "                scale = (x_np.max() - x_np.min()) / (self.qmax - self.qmin)\n",
    "                scale = scale if scale > 0 else 1.0\n",
    "        \n",
    "        # Quantize\n",
    "        x_scaled = x_np / scale\n",
    "        x_rounded = np.round(x_scaled)\n",
    "        x_clipped = np.clip(x_rounded, self.qmin, self.qmax)\n",
    "        \n",
    "        # Track what was saturated\n",
    "        was_saturated = (x_rounded != x_clipped)\n",
    "        \n",
    "        # Dequantize\n",
    "        x_deq = x_clipped * scale\n",
    "        \n",
    "        # Decompose error\n",
    "        round_error = (x_rounded * scale - x_np)  # Error from rounding alone\n",
    "        clip_error = (x_clipped - x_rounded) * scale  # Additional error from clipping\n",
    "        \n",
    "        return x_deq, scale, was_saturated, round_error, clip_error\n",
    "    \n",
    "    def analyze_relu_error(self, x_fp, x_quant, error_before_relu):\n",
    "        \"\"\"\n",
    "        Analyze how ReLU interacts with quantization error.\n",
    "        \n",
    "        Three cases:\n",
    "        1. Both positive: ReLU is identity, error passes through linearly\n",
    "        2. Both negative: ReLU zeros both, error is zeroed (good!)\n",
    "        3. Different signs: Nonlinear interaction (bad!)\n",
    "        \"\"\"\n",
    "        x_fp_np = x_fp.numpy() if isinstance(x_fp, torch.Tensor) else x_fp\n",
    "        x_q_np = x_quant.numpy() if isinstance(x_quant, torch.Tensor) else x_quant\n",
    "        \n",
    "        # After ReLU\n",
    "        y_fp = np.maximum(x_fp_np, 0)\n",
    "        y_q = np.maximum(x_q_np, 0)\n",
    "        \n",
    "        # Actual error after ReLU\n",
    "        actual_error = y_q - y_fp\n",
    "        \n",
    "        # What the error WOULD be if ReLU were linear (identity)\n",
    "        linear_error = error_before_relu\n",
    "        \n",
    "        # The nonlinear part is the difference\n",
    "        nonlinear_error = actual_error - linear_error\n",
    "        \n",
    "        # Count flips: where ReLU decision differs\n",
    "        fp_positive = x_fp_np > 0\n",
    "        q_positive = x_q_np > 0\n",
    "        n_flipped = np.sum(fp_positive != q_positive)\n",
    "        \n",
    "        return actual_error, linear_error, nonlinear_error, n_flipped\n",
    "    \n",
    "    def forward_with_decomposition(self, model, x) -> List[ErrorDecomposition]:\n",
    "        \"\"\"\n",
    "        Run forward pass tracking all error components.\n",
    "        Returns list of ErrorDecomposition for each layer.\n",
    "        \"\"\"\n",
    "        decompositions = []\n",
    "        \n",
    "        x_fp = x.clone().float()\n",
    "        x_q = x.clone().float()\n",
    "        \n",
    "        for layer_idx, layer in enumerate(model.layers[:-1]):\n",
    "            W = layer.weight.detach().numpy()\n",
    "            b = layer.bias.detach().numpy() if layer.bias is not None else np.zeros(W.shape[0])\n",
    "            \n",
    "            # === FP32 path ===\n",
    "            x_fp_pre_relu = (x_fp.numpy() @ W.T + b)\n",
    "            \n",
    "            # === Quantized path with decomposition ===\n",
    "            \n",
    "            # Step 1: Quantize weights\n",
    "            W_q, w_scale, _, w_round_err, w_clip_err = self.quantize(W)\n",
    "            b_q, b_scale, _, _, _ = self.quantize(b)\n",
    "            \n",
    "            # Weight quantization error contribution\n",
    "            # (W_q - W) @ x\n",
    "            weight_quant_error = x_q.numpy() @ (W_q - W).T\n",
    "            \n",
    "            # Step 2: Compute pre-activation with quantized weights\n",
    "            x_q_pre_act = x_q.numpy() @ W_q.T + b_q\n",
    "            \n",
    "            # Step 3: Quantize activations\n",
    "            x_q_post_quant, act_scale, was_saturated, round_err, clip_err = self.quantize(x_q_pre_act)\n",
    "            \n",
    "            # Current accumulated error (before ReLU)\n",
    "            error_pre_relu = x_q_post_quant - x_fp_pre_relu\n",
    "            \n",
    "            # Step 4: Apply ReLU and analyze interaction\n",
    "            x_fp_post_relu = np.maximum(x_fp_pre_relu, 0)\n",
    "            x_q_post_relu = np.maximum(x_q_post_quant, 0)\n",
    "            \n",
    "            actual_error, linear_error_component, nonlinear_relu_error, n_flipped = \\\n",
    "                self.analyze_relu_error(x_fp_pre_relu, x_q_post_quant, error_pre_relu)\n",
    "            \n",
    "            # Build decomposition\n",
    "            decomp = ErrorDecomposition(\n",
    "                layer_idx=layer_idx,\n",
    "                weight_quant_error=weight_quant_error.flatten(),\n",
    "                activation_round_error=round_err.flatten(),\n",
    "                saturation_error=clip_err.flatten(),\n",
    "                relu_interaction_error=nonlinear_relu_error.flatten(),\n",
    "                n_saturated=int(was_saturated.sum()),\n",
    "                n_relu_flipped=n_flipped,\n",
    "            )\n",
    "            decompositions.append(decomp)\n",
    "            \n",
    "            # Update for next layer\n",
    "            x_fp = torch.tensor(x_fp_post_relu, dtype=torch.float32)\n",
    "            x_q = torch.tensor(x_q_post_relu, dtype=torch.float32)\n",
    "        \n",
    "        return decompositions\n",
    "\n",
    "\n",
    "def visualize_error_decomposition(decompositions: List[ErrorDecomposition]):\n",
    "    \"\"\"\n",
    "    Create visualization showing linear vs nonlinear error at each layer.\n",
    "    \"\"\"\n",
    "    n_layers = len(decompositions)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Data for plotting\n",
    "    layers = [d.layer_idx for d in decompositions]\n",
    "    weight_err = [np.linalg.norm(d.weight_quant_error) for d in decompositions]\n",
    "    round_err = [np.linalg.norm(d.activation_round_error) for d in decompositions]\n",
    "    sat_err = [np.linalg.norm(d.saturation_error) for d in decompositions]\n",
    "    relu_err = [np.linalg.norm(d.relu_interaction_error) for d in decompositions]\n",
    "    \n",
    "    # Plot 1: Stacked bar of error components\n",
    "    ax = axes[0, 0]\n",
    "    width = 0.6\n",
    "    ax.bar(layers, weight_err, width, label='Weight quant (linear)', color='steelblue')\n",
    "    ax.bar(layers, round_err, width, bottom=weight_err, label='Rounding (linear)', color='lightblue')\n",
    "    bottom_nonlin = np.array(weight_err) + np.array(round_err)\n",
    "    ax.bar(layers, sat_err, width, bottom=bottom_nonlin, label='Saturation (nonlinear)', color='coral')\n",
    "    ax.bar(layers, relu_err, width, bottom=bottom_nonlin + np.array(sat_err), \n",
    "           label='ReLU flip (nonlinear)', color='red')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Error norm')\n",
    "    ax.set_title('Error Decomposition by Layer')\n",
    "    ax.legend()\n",
    "    ax.set_xticks(layers)\n",
    "    \n",
    "    # Plot 2: Linear fraction over layers\n",
    "    ax = axes[0, 1]\n",
    "    linear_frac = [d.linear_fraction for d in decompositions]\n",
    "    colors = ['green' if f > 0.8 else 'orange' if f > 0.5 else 'red' for f in linear_frac]\n",
    "    ax.bar(layers, linear_frac, color=colors, edgecolor='black')\n",
    "    ax.axhline(0.5, color='gray', linestyle='--', label='50% threshold')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Fraction of error that is linear')\n",
    "    ax.set_title('Compensatable Error Fraction\\n(green=mostly fixable, red=mostly unfixable)')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(layers)\n",
    "    \n",
    "    # Plot 3: Saturation events\n",
    "    ax = axes[1, 0]\n",
    "    n_sat = [d.n_saturated for d in decompositions]\n",
    "    n_flip = [d.n_relu_flipped for d in decompositions]\n",
    "    x_pos = np.arange(n_layers)\n",
    "    width = 0.35\n",
    "    ax.bar(x_pos - width/2, n_sat, width, label='Saturated activations', color='coral')\n",
    "    ax.bar(x_pos + width/2, n_flip, width, label='ReLU flips', color='purple')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Nonlinear Events per Layer')\n",
    "    ax.legend()\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(layers)\n",
    "    \n",
    "    # Plot 4: Cumulative error growth\n",
    "    ax = axes[1, 1]\n",
    "    total_err = [np.linalg.norm(d.total_error) for d in decompositions]\n",
    "    linear_err = [np.linalg.norm(d.total_linear_error) for d in decompositions]\n",
    "    nonlin_err = [np.linalg.norm(d.total_nonlinear_error) for d in decompositions]\n",
    "    \n",
    "    ax.plot(layers, total_err, 'k-o', linewidth=2, markersize=8, label='Total error')\n",
    "    ax.plot(layers, linear_err, 'b--s', linewidth=2, markersize=6, label='Linear component')\n",
    "    ax.plot(layers, nonlin_err, 'r--^', linewidth=2, markersize=6, label='Nonlinear component')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Error norm')\n",
    "    ax.set_title('Error Growth Through Network')\n",
    "    ax.legend()\n",
    "    ax.set_xticks(layers)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_relu_flip_regions(W, b, bits=8, n_grid=200):\n",
    "    \"\"\"\n",
    "    Visualize where in input space ReLU decisions flip due to quantization.\n",
    "    These are the \"danger zones\" where nonlinear error is introduced.\n",
    "    \"\"\"\n",
    "    W_q, _ = quantize_matrix_np(W, bits)\n",
    "    b_q, _ = quantize_matrix_np(b.reshape(1, -1), bits)\n",
    "    b_q = b_q.flatten()\n",
    "    \n",
    "    # Create grid\n",
    "    x = np.linspace(-2, 2, n_grid)\n",
    "    xx, yy = np.meshgrid(x, x)\n",
    "    points = np.stack([xx.flatten(), yy.flatten()], axis=1)\n",
    "    \n",
    "    # Pre-ReLU activations\n",
    "    pre_relu_fp = points @ W.T + b\n",
    "    pre_relu_q = points @ W_q.T + b_q\n",
    "    \n",
    "    # For each output dimension, find flip regions\n",
    "    n_outputs = W.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_outputs + 1, figsize=(5 * (n_outputs + 1), 5))\n",
    "    \n",
    "    total_flips = np.zeros((n_grid, n_grid))\n",
    "    \n",
    "    for i in range(n_outputs):\n",
    "        fp_positive = pre_relu_fp[:, i] > 0\n",
    "        q_positive = pre_relu_q[:, i] > 0\n",
    "        flipped = (fp_positive != q_positive).reshape(n_grid, n_grid)\n",
    "        total_flips += flipped\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.contourf(xx, yy, flipped.astype(float), levels=[0.5, 1.5], colors=['red'], alpha=0.5)\n",
    "        ax.contour(xx, yy, pre_relu_fp[:, i].reshape(n_grid, n_grid), \n",
    "                  levels=[0], colors='blue', linewidths=2, linestyles='--')\n",
    "        ax.contour(xx, yy, pre_relu_q[:, i].reshape(n_grid, n_grid),\n",
    "                  levels=[0], colors='red', linewidths=2)\n",
    "        ax.set_title(f'Output dim {i}\\nBlue=FP32 boundary, Red=INT{bits}')\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    # Total flip regions\n",
    "    ax = axes[-1]\n",
    "    im = ax.contourf(xx, yy, total_flips, levels=np.arange(n_outputs + 2) - 0.5, cmap='Reds')\n",
    "    plt.colorbar(im, ax=ax, label='# dimensions flipped')\n",
    "    ax.set_title('Total ReLU flip regions\\n(darker = more danger)')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def quantize_matrix_np(W, bits=8):\n",
    "    \"\"\"Numpy version of matrix quantization\"\"\"\n",
    "    qmax = 2**(bits-1) - 1\n",
    "    scale = np.abs(W).max() / qmax if np.abs(W).max() > 0 else 1.0\n",
    "    W_q = np.round(W / scale) * scale\n",
    "    return W_q, scale\n",
    "\n",
    "\n",
    "def analyze_correctability(model, x_samples, bits=8):\n",
    "    \"\"\"\n",
    "    For a batch of samples, compute:\n",
    "    1. Total error at output\n",
    "    2. Best possible linear correction (least squares)\n",
    "    3. Residual error after correction (the uncorrectable part)\n",
    "    \n",
    "    This tells you: if you had an oracle linear correction layer,\n",
    "    how much error would remain?\n",
    "    \"\"\"\n",
    "    tracker = ErrorTracker(bits=bits)\n",
    "    \n",
    "    # Collect outputs\n",
    "    y_fp_list = []\n",
    "    y_q_list = []\n",
    "    \n",
    "    for x in x_samples:\n",
    "        x = x.unsqueeze(0) if x.dim() == 1 else x\n",
    "        \n",
    "        # FP32 forward\n",
    "        y_fp = x.clone()\n",
    "        for layer in model.layers:\n",
    "            y_fp = layer(y_fp)\n",
    "            if layer != model.layers[-1]:\n",
    "                y_fp = torch.relu(y_fp)\n",
    "        \n",
    "        # Quantized forward (simplified)\n",
    "        y_q = x.clone()\n",
    "        for layer in model.layers:\n",
    "            W_q, _ = quantize_matrix_np(layer.weight.detach().numpy(), bits)\n",
    "            b_q, _ = quantize_matrix_np(layer.bias.detach().numpy(), bits)\n",
    "            y_q_np = y_q.numpy() @ W_q.T + b_q\n",
    "            y_q_np, _, _, _, _ = tracker.quantize(y_q_np)\n",
    "            if layer != model.layers[-1]:\n",
    "                y_q_np = np.maximum(y_q_np, 0)\n",
    "            y_q = torch.tensor(y_q_np, dtype=torch.float32)\n",
    "        \n",
    "        y_fp_list.append(y_fp.detach().numpy().flatten())\n",
    "        y_q_list.append(y_q.numpy().flatten())\n",
    "    \n",
    "    Y_fp = np.array(y_fp_list)\n",
    "    Y_q = np.array(y_q_list)\n",
    "    \n",
    "    # Error before correction\n",
    "    error_before = Y_q - Y_fp\n",
    "    error_norm_before = np.linalg.norm(error_before, axis=1).mean()\n",
    "    \n",
    "    # Best linear correction: find A, b such that Y_q @ A + b ≈ Y_fp\n",
    "    # Least squares: A = (Y_q^T Y_q)^{-1} Y_q^T Y_fp\n",
    "    Y_q_aug = np.hstack([Y_q, np.ones((Y_q.shape[0], 1))])  # Add bias term\n",
    "    \n",
    "    try:\n",
    "        correction_params, residuals, rank, s = np.linalg.lstsq(Y_q_aug, Y_fp, rcond=None)\n",
    "        Y_corrected = Y_q_aug @ correction_params\n",
    "        error_after = Y_corrected - Y_fp\n",
    "        error_norm_after = np.linalg.norm(error_after, axis=1).mean()\n",
    "    except:\n",
    "        error_norm_after = error_norm_before\n",
    "        correction_params = None\n",
    "    \n",
    "    correctability = 1 - (error_norm_after / error_norm_before) if error_norm_before > 0 else 1.0\n",
    "    \n",
    "    return {\n",
    "        'error_before_correction': error_norm_before,\n",
    "        'error_after_correction': error_norm_after,\n",
    "        'correctability': correctability,  # 1.0 = fully correctable, 0.0 = not at all\n",
    "        'correction_params': correction_params\n",
    "    }\n",
    "\n",
    "\n",
    "# ============ Demo ============\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, dims=[2, 8, 8, 4, 1]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleNet(dims=[2, 8, 8, 4, 1])\n",
    "    for layer in model.layers:\n",
    "        nn.init.xavier_uniform_(layer.weight, gain=2.0)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ERROR DECOMPOSITION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Single sample decomposition\n",
    "    tracker = ErrorTracker(bits=8)\n",
    "    x = torch.randn(1, 2)\n",
    "    decompositions = tracker.forward_with_decomposition(model, x)\n",
    "    \n",
    "    print(\"\\nPer-layer error breakdown:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Layer':<8} {'Weight Q':<12} {'Round':<12} {'Saturate':<12} {'ReLU flip':<12} {'Linear %':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for d in decompositions:\n",
    "        s = d.summary()\n",
    "        print(f\"{s['layer']:<8} {s['weight_quant']:<12.4f} {s['round']:<12.4f} \"\n",
    "              f\"{s['saturation']:<12.4f} {s['relu_flip']:<12.4f} {s['linear_fraction']*100:<10.1f}%\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_error_decomposition(decompositions)\n",
    "    plt.savefig('plots/error_decomposition.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RELU FLIP REGIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize ReLU flip regions for first layer\n",
    "    W = model.layers[0].weight.detach().numpy()\n",
    "    b = model.layers[0].bias.detach().numpy()\n",
    "    fig = visualize_relu_flip_regions(W, b, bits=4)  # Use 4-bit to exaggerate\n",
    "    plt.savefig('plots/relu_flip_regions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CORRECTABILITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test correctability at different bit widths\n",
    "    x_samples = torch.randn(500, 2)\n",
    "    \n",
    "    print(\"\\nHow much error can be fixed by a linear correction layer?\")\n",
    "    print(\"-\" * 50)\n",
    "    for bits in [8, 6, 4, 2]:\n",
    "        result = analyze_correctability(model, x_samples, bits=bits)\n",
    "        print(f\"{bits}-bit: {result['correctability']*100:.1f}% correctable \"\n",
    "              f\"(error {result['error_before_correction']:.4f} → {result['error_after_correction']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eac1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-aware-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
