{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d0d1b0",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "## The Key Relationships\n",
    "\n",
    "### 1. **Volume ↔ Entropy (Linear Case)**\n",
    "\n",
    "For a uniform distribution over a region:\n",
    "```\n",
    "H = log₂(Volume)\n",
    "```\n",
    "\n",
    "For a Gaussian (what errors often converge to):\n",
    "```\n",
    "H = (n/2) log₂(2πe) + (1/2) log₂(det(Σ))\n",
    "    └─────────────┘   └─────────────────┘\n",
    "     dimension term      volume term\n",
    "```\n",
    "\n",
    "The determinant of the covariance IS the volume of the uncertainty ellipsoid.\n",
    "\n",
    "### 2. **Linear Transforms Scale Both Equally**\n",
    "```\n",
    "H_after = H_before + log|det(W)|\n",
    "Vol_after = Vol_before × |det(W)|\n",
    "\n",
    "So: ΔH = log(Vol_after / Vol_before)\n",
    "```\n",
    "\n",
    "This is why spectral analysis (singular values of W) tells you about error growth — it's also telling you about entropy growth.\n",
    "\n",
    "### 3. **Nonlinear Ops Break the Relationship**\n",
    "\n",
    "ReLU and clipping both:\n",
    "- Reduce volume (geometrically)\n",
    "- Reduce entropy (information-theoretically)\n",
    "\n",
    "But this isn't \"free precision\" — it's **information destruction**. Multiple inputs map to the same output.\n",
    "\n",
    "### 4. **The Saturation Catastrophe in Entropy Terms**\n",
    "\n",
    "When you clip values:\n",
    "```\n",
    "Before: H_before = some finite value\n",
    "After:  H_after = H_before - log₂(n_collapsed)\n",
    "\n",
    "where n_collapsed = number of distinct values that got mapped to the clamp limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GeometricEntropyTracker:\n",
    "    \"\"\"\n",
    "    Track both geometric (volume-based) and information-theoretic\n",
    "    (entropy-based) measures of quantization error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bits=8):\n",
    "        self.bits = bits\n",
    "        self.delta = 1.0 / (2 ** bits)  # Quantization step size\n",
    "    \n",
    "    def hypercube_volume(self, n_dims):\n",
    "        \"\"\"Volume of n-dimensional quantization error hypercube\"\"\"\n",
    "        return self.delta ** n_dims\n",
    "    \n",
    "    def hypercube_entropy(self, n_dims):\n",
    "        \"\"\"\n",
    "        Entropy of uniform distribution over hypercube.\n",
    "        H = log(volume) for continuous uniform.\n",
    "        \"\"\"\n",
    "        return n_dims * np.log2(self.delta)  # Will be negative\n",
    "    \n",
    "    def parallelepiped_volume(self, W, base_delta=None):\n",
    "        \"\"\"\n",
    "        After linear transform W, hypercube becomes parallelepiped.\n",
    "        Volume scales by |det(W)|.\n",
    "        \"\"\"\n",
    "        delta = base_delta if base_delta else self.delta\n",
    "        n_dims = W.shape[0]\n",
    "        \n",
    "        # For non-square W, use product of singular values\n",
    "        if W.shape[0] != W.shape[1]:\n",
    "            singular_values = np.linalg.svd(W, compute_uv=False)\n",
    "            scale = np.prod(singular_values[:min(W.shape)])\n",
    "        else:\n",
    "            scale = np.abs(np.linalg.det(W))\n",
    "        \n",
    "        return scale * (delta ** n_dims)\n",
    "    \n",
    "    def entropy_after_linear(self, W, base_entropy=None):\n",
    "        \"\"\"\n",
    "        Entropy change under linear transform.\n",
    "        H_after = H_before + log|det(W)|\n",
    "        \"\"\"\n",
    "        if base_entropy is None:\n",
    "            base_entropy = self.hypercube_entropy(W.shape[1])\n",
    "        \n",
    "        if W.shape[0] != W.shape[1]:\n",
    "            singular_values = np.linalg.svd(W, compute_uv=False)\n",
    "            log_det = np.sum(np.log2(singular_values[:min(W.shape)] + 1e-10))\n",
    "        else:\n",
    "            sign, log_det = np.linalg.slogdet(W)\n",
    "            log_det = log_det / np.log(2)  # Convert to bits\n",
    "        \n",
    "        return base_entropy + log_det\n",
    "    \n",
    "    def estimate_entropy_from_samples(self, samples, n_bins=50):\n",
    "        \"\"\"\n",
    "        Empirically estimate entropy from samples using histogram.\n",
    "        \"\"\"\n",
    "        if samples.ndim == 1:\n",
    "            hist, _ = np.histogram(samples, bins=n_bins, density=True)\n",
    "            hist = hist[hist > 0]  # Remove zeros\n",
    "            bin_width = (samples.max() - samples.min()) / n_bins\n",
    "            # Differential entropy approximation\n",
    "            return -np.sum(hist * np.log2(hist + 1e-10)) * bin_width\n",
    "        else:\n",
    "            # For multivariate, use covariance-based estimate (assumes Gaussian-ish)\n",
    "            cov = np.cov(samples.T)\n",
    "            sign, log_det = np.linalg.slogdet(cov)\n",
    "            n = samples.shape[1]\n",
    "            # Entropy of multivariate Gaussian: 0.5 * log((2πe)^n |Σ|)\n",
    "            return 0.5 * (n * np.log2(2 * np.pi * np.e) + log_det / np.log(2))\n",
    "    \n",
    "    def information_loss_from_clipping(self, samples, clip_min, clip_max):\n",
    "        \"\"\"\n",
    "        Estimate information lost when values are clipped.\n",
    "        \n",
    "        Information loss occurs when multiple input values map to the same output.\n",
    "        \"\"\"\n",
    "        n_total = len(samples)\n",
    "        n_clipped_high = np.sum(samples > clip_max)\n",
    "        n_clipped_low = np.sum(samples < clip_min)\n",
    "        n_clipped = n_clipped_high + n_clipped_low\n",
    "        \n",
    "        if n_clipped == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Before clipping: each value is distinguishable\n",
    "        # After clipping: all values above clip_max are indistinguishable\n",
    "        \n",
    "        # Rough estimate: bits lost ≈ log2(n_clipped) for each clipping region\n",
    "        # because n_clipped values collapse to 1\n",
    "        \n",
    "        bits_lost = 0\n",
    "        if n_clipped_high > 1:\n",
    "            bits_lost += np.log2(n_clipped_high)\n",
    "        if n_clipped_low > 1:\n",
    "            bits_lost += np.log2(n_clipped_low)\n",
    "        \n",
    "        return bits_lost\n",
    "    \n",
    "    def relu_entropy_reduction(self, samples):\n",
    "        \"\"\"\n",
    "        Estimate entropy change from ReLU.\n",
    "        \n",
    "        ReLU maps all negative values to 0, collapsing a region of input space.\n",
    "        \"\"\"\n",
    "        n_total = len(samples)\n",
    "        n_negative = np.sum(samples < 0)\n",
    "        n_positive = n_total - n_negative\n",
    "        \n",
    "        if n_negative == 0:\n",
    "            return 0.0  # No change\n",
    "        \n",
    "        # Entropy before: H(samples)\n",
    "        H_before = self.estimate_entropy_from_samples(samples)\n",
    "        \n",
    "        # Entropy after: H(relu(samples))\n",
    "        samples_after = np.maximum(samples, 0)\n",
    "        \n",
    "        # The zeros are all indistinguishable, so we have:\n",
    "        # - A point mass at 0 with probability n_negative/n_total\n",
    "        # - A continuous distribution for positive values\n",
    "        \n",
    "        p_zero = n_negative / n_total\n",
    "        p_positive = n_positive / n_total\n",
    "        \n",
    "        if n_positive > 1:\n",
    "            positive_samples = samples[samples > 0]\n",
    "            H_positive_part = self.estimate_entropy_from_samples(positive_samples)\n",
    "        else:\n",
    "            H_positive_part = 0\n",
    "        \n",
    "        # Mixture entropy (approximation)\n",
    "        # H ≈ p_pos * H_positive + H(p_zero, p_pos)\n",
    "        if p_zero > 0 and p_positive > 0:\n",
    "            binary_entropy = -p_zero * np.log2(p_zero) - p_positive * np.log2(p_positive)\n",
    "        else:\n",
    "            binary_entropy = 0\n",
    "        \n",
    "        H_after = p_positive * H_positive_part + binary_entropy\n",
    "        \n",
    "        return H_before - H_after  # Entropy reduction\n",
    "\n",
    "\n",
    "def visualize_entropy_geometry_connection():\n",
    "    \"\"\"\n",
    "    Visualize how volume and entropy relate through transformations.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Initial error distribution (uniform in square)\n",
    "    n_samples = 5000\n",
    "    errors = np.random.uniform(-0.5, 0.5, size=(n_samples, 2))\n",
    "    \n",
    "    tracker = GeometricEntropyTracker(bits=8)\n",
    "    \n",
    "    # Stage 1: Initial hypercube\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(errors[:, 0], errors[:, 1], alpha=0.3, s=1)\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    vol = 1.0  # Unit square scaled by delta\n",
    "    H = tracker.estimate_entropy_from_samples(errors)\n",
    "    ax.set_title(f'Initial Error Hypercube\\nVolume ∝ 1.0, Entropy ≈ {H:.2f} bits')\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Stage 2: After linear transform (rotation + scale)\n",
    "    W = np.array([[1.5, 0.5], [-0.3, 1.2]])\n",
    "    errors_transformed = errors @ W.T\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(errors_transformed[:, 0], errors_transformed[:, 1], alpha=0.3, s=1)\n",
    "    ax.set_aspect('equal')\n",
    "    vol_after_W = np.abs(np.linalg.det(W))\n",
    "    H_after_W = tracker.estimate_entropy_from_samples(errors_transformed)\n",
    "    ax.set_title(f'After Linear W\\nVolume × {vol_after_W:.2f}, Entropy ≈ {H_after_W:.2f} bits')\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Stage 3: After ReLU (shift then clip)\n",
    "    shift = np.array([0.3, 0.2])\n",
    "    errors_shifted = errors_transformed + shift\n",
    "    errors_relu = np.maximum(errors_shifted, 0)\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    ax.scatter(errors_relu[:, 0], errors_relu[:, 1], alpha=0.3, s=1)\n",
    "    ax.set_aspect('equal')\n",
    "    n_clipped = np.sum((errors_shifted < 0).any(axis=1))\n",
    "    H_after_relu = tracker.estimate_entropy_from_samples(errors_relu)\n",
    "    ax.set_title(f'After ReLU\\n{n_clipped} points clipped, Entropy ≈ {H_after_relu:.2f} bits')\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Bottom row: 1D marginals showing the distributions\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(errors[:, 0], bins=50, density=True, alpha=0.7)\n",
    "    ax.set_title('Initial (dim 0)')\n",
    "    ax.set_xlabel('Error')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.hist(errors_transformed[:, 0], bins=50, density=True, alpha=0.7)\n",
    "    ax.set_title('After W (dim 0)')\n",
    "    ax.set_xlabel('Error')\n",
    "    \n",
    "    ax = axes[1, 2]\n",
    "    ax.hist(errors_relu[:, 0], bins=50, density=True, alpha=0.7)\n",
    "    # Note the spike at 0\n",
    "    ax.set_title('After ReLU (dim 0)\\nNote spike at 0 = lost information')\n",
    "    ax.set_xlabel('Error')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def analyze_layer_by_layer_entropy(model, x_samples, bits=8):\n",
    "    \"\"\"\n",
    "    Track entropy of the error distribution through each layer.\n",
    "    Compare geometric prediction vs empirical measurement.\n",
    "    \"\"\"\n",
    "    tracker = GeometricEntropyTracker(bits=bits)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Get all layer outputs for FP32\n",
    "    fp_activations = []\n",
    "    q_activations = []\n",
    "    \n",
    "    for x in x_samples:\n",
    "        x_fp = x.unsqueeze(0).float()\n",
    "        x_q = x.unsqueeze(0).float()\n",
    "        \n",
    "        fp_acts = [x_fp.numpy().flatten()]\n",
    "        q_acts = [x_q.numpy().flatten()]\n",
    "        \n",
    "        for layer in model.layers[:-1]:\n",
    "            # FP path\n",
    "            x_fp = layer(x_fp)\n",
    "            fp_acts.append(x_fp.detach().numpy().flatten())\n",
    "            x_fp = torch.relu(x_fp)\n",
    "            fp_acts.append(x_fp.detach().numpy().flatten())\n",
    "            \n",
    "            # Quantized path\n",
    "            W_q = quantize_simple(layer.weight.detach().numpy(), bits)\n",
    "            b_q = quantize_simple(layer.bias.detach().numpy(), bits)\n",
    "            x_q_np = x_q.numpy() @ W_q.T + b_q\n",
    "            x_q_np = quantize_simple(x_q_np, bits)\n",
    "            q_acts.append(x_q_np.flatten())\n",
    "            x_q_np = np.maximum(x_q_np, 0)\n",
    "            q_acts.append(x_q_np.flatten())\n",
    "            x_q = torch.tensor(x_q_np, dtype=torch.float32)\n",
    "        \n",
    "        fp_activations.append(fp_acts)\n",
    "        q_activations.append(q_acts)\n",
    "    \n",
    "    # Compute errors at each stage\n",
    "    n_stages = len(fp_activations[0])\n",
    "    \n",
    "    for stage in range(n_stages):\n",
    "        fp_vals = np.array([fa[stage] for fa in fp_activations])\n",
    "        q_vals = np.array([qa[stage] for qa in q_activations])\n",
    "        errors = q_vals - fp_vals\n",
    "        \n",
    "        # Empirical entropy of error\n",
    "        H_empirical = tracker.estimate_entropy_from_samples(errors)\n",
    "        \n",
    "        # Volume estimate (using covariance determinant as proxy)\n",
    "        if errors.shape[1] > 1:\n",
    "            cov = np.cov(errors.T)\n",
    "            sign, logdet = np.linalg.slogdet(cov)\n",
    "            vol_proxy = np.exp(logdet / 2)  # sqrt(det(cov)) ~ \"radius\"\n",
    "        else:\n",
    "            vol_proxy = np.std(errors)\n",
    "        \n",
    "        results.append({\n",
    "            'stage': stage,\n",
    "            'entropy': H_empirical,\n",
    "            'volume_proxy': vol_proxy,\n",
    "            'error_norm_mean': np.linalg.norm(errors, axis=1).mean(),\n",
    "            'n_dims': errors.shape[1]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def quantize_simple(x, bits):\n",
    "    \"\"\"Simple symmetric quantization\"\"\"\n",
    "    qmax = 2**(bits-1) - 1\n",
    "    scale = np.abs(x).max() / qmax if np.abs(x).max() > 0 else 1.0\n",
    "    return np.round(x / scale) * scale\n",
    "\n",
    "\n",
    "def visualize_entropy_vs_geometry(layer_results):\n",
    "    \"\"\"Plot entropy and geometric measures together\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    stages = [r['stage'] for r in layer_results]\n",
    "    entropies = [r['entropy'] for r in layer_results]\n",
    "    volumes = [r['volume_proxy'] for r in layer_results]\n",
    "    error_norms = [r['error_norm_mean'] for r in layer_results]\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.plot(stages, entropies, 'o-', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Stage (layer × 2 for pre/post ReLU)')\n",
    "    ax.set_ylabel('Entropy (bits)')\n",
    "    ax.set_title('Error Entropy Through Network')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.plot(stages, volumes, 's-', linewidth=2, markersize=8, color='green')\n",
    "    ax.set_xlabel('Stage')\n",
    "    ax.set_ylabel('Volume proxy (sqrt det cov)')\n",
    "    ax.set_title('Error \"Volume\" Through Network')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[2]\n",
    "    # Entropy vs log(volume) should be roughly linear\n",
    "    ax.scatter(np.log(np.array(volumes) + 1e-10), entropies, s=50)\n",
    "    ax.set_xlabel('log(Volume proxy)')\n",
    "    ax.set_ylabel('Entropy')\n",
    "    ax.set_title('Entropy vs log(Volume)\\nShould be roughly linear')\n",
    "    \n",
    "    # Fit line\n",
    "    valid = np.array(volumes) > 1e-10\n",
    "    if valid.sum() > 2:\n",
    "        z = np.polyfit(np.log(np.array(volumes)[valid]), np.array(entropies)[valid], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(np.log(min(np.array(volumes)[valid])), \n",
    "                            np.log(max(np.array(volumes)[valid])), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r--', label=f'slope={z[0]:.2f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Demo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, dims=[2, 4, 4, 2]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENTROPY-GEOMETRY CONNECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize basic connection\n",
    "    fig = visualize_entropy_geometry_connection()\n",
    "    plt.savefig('plots/entropy_geometry_basic.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Track through network\n",
    "    model = SimpleNet(dims=[2, 4, 4, 2])\n",
    "    for layer in model.layers:\n",
    "        nn.init.xavier_uniform_(layer.weight, gain=1.5)\n",
    "    \n",
    "    x_samples = torch.randn(500, 2)\n",
    "    results = analyze_layer_by_layer_entropy(model, x_samples, bits=8)\n",
    "    \n",
    "    print(\"\\nEntropy and volume through layers:\")\n",
    "    print(\"-\" * 60)\n",
    "    for r in results:\n",
    "        print(f\"Stage {r['stage']}: H={r['entropy']:.2f} bits, \"\n",
    "              f\"vol={r['volume_proxy']:.4f}, err_norm={r['error_norm_mean']:.4f}\")\n",
    "    \n",
    "    fig = visualize_entropy_vs_geometry(results)\n",
    "    plt.savefig('plots/entropy_vs_geometry.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "class ErrorShapeTracker:\n",
    "    \"\"\"\n",
    "    Track the geometry of the error region through linear layers.\n",
    "    \n",
    "    Start with a hypercube (from initial quantization),\n",
    "    transform through layers, observe how shape evolves.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dims, bits=8):\n",
    "        self.dims = dims\n",
    "        self.bits = bits\n",
    "        self.delta = 1.0 / (2 ** bits)  # Quantization step\n",
    "        \n",
    "    def initial_error_hypercube(self, n_samples=1000):\n",
    "        \"\"\"\n",
    "        Initial error is uniform in [-delta/2, delta/2]^d\n",
    "        Represent by sampling vertices or interior points.\n",
    "        \"\"\"\n",
    "        # For low dims, can use actual vertices\n",
    "        if self.dims <= 10:\n",
    "            # Hypercube has 2^d vertices\n",
    "            from itertools import product\n",
    "            vertices = np.array(list(product([-1, 1], repeat=self.dims))) * self.delta / 2\n",
    "            return vertices\n",
    "        else:\n",
    "            # Sample uniformly from hypercube\n",
    "            return np.random.uniform(-self.delta/2, self.delta/2, size=(n_samples, self.dims))\n",
    "    \n",
    "    def transform_linear(self, error_points, W):\n",
    "        \"\"\"\n",
    "        Linear transformation: error' = W @ error\n",
    "        \"\"\"\n",
    "        return error_points @ W.T\n",
    "    \n",
    "    def add_quantization_error(self, error_points, new_delta=None):\n",
    "        \"\"\"\n",
    "        After quantizing activations, we add another hypercube of error.\n",
    "        This is a Minkowski sum.\n",
    "        \n",
    "        For point clouds: sample from the sum.\n",
    "        \"\"\"\n",
    "        if new_delta is None:\n",
    "            new_delta = self.delta\n",
    "            \n",
    "        n_points = len(error_points)\n",
    "        new_dims = error_points.shape[1]\n",
    "        \n",
    "        # Approximate Minkowski sum by adding random offset from new hypercube\n",
    "        offsets = np.random.uniform(-new_delta/2, new_delta/2, size=(n_points, new_dims))\n",
    "        return error_points + offsets\n",
    "    \n",
    "    def apply_relu_to_error(self, error_points, activation_center):\n",
    "        \"\"\"\n",
    "        ReLU effect on error region.\n",
    "        \n",
    "        If activation + error < 0, gets clipped.\n",
    "        This depends on where the \"true\" activation is.\n",
    "        \n",
    "        activation_center: the FP32 activation value (or typical value)\n",
    "        error_points: the error region around it\n",
    "        \"\"\"\n",
    "        # Points in activation space\n",
    "        activation_points = activation_center + error_points\n",
    "        \n",
    "        # After ReLU\n",
    "        activation_post_relu = np.maximum(activation_points, 0)\n",
    "        \n",
    "        # New center\n",
    "        center_post_relu = np.maximum(activation_center, 0)\n",
    "        \n",
    "        # Error after ReLU\n",
    "        error_post_relu = activation_post_relu - center_post_relu\n",
    "        \n",
    "        return error_post_relu, center_post_relu\n",
    "    \n",
    "    def apply_saturation(self, error_points, activation_center, qmin, qmax):\n",
    "        \"\"\"\n",
    "        Clipping effect on error region.\n",
    "        \"\"\"\n",
    "        activation_points = activation_center + error_points\n",
    "        activation_clipped = np.clip(activation_points, qmin, qmax)\n",
    "        center_clipped = np.clip(activation_center, qmin, qmax)\n",
    "        error_clipped = activation_clipped - center_clipped\n",
    "        \n",
    "        return error_clipped, center_clipped\n",
    "    \n",
    "    def measure_shape(self, error_points):\n",
    "        \"\"\"\n",
    "        Compute shape statistics:\n",
    "        - Volume (via covariance determinant)\n",
    "        - Principal axes (via SVD)\n",
    "        - Bounding box\n",
    "        - Convexity measure\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # Covariance\n",
    "        cov = np.cov(error_points.T)\n",
    "        \n",
    "        # Volume proxy\n",
    "        sign, logdet = np.linalg.slogdet(cov)\n",
    "        stats['log_volume'] = logdet / 2  # sqrt(det) for \"radius\"\n",
    "        \n",
    "        # Principal axes (singular values of centered points)\n",
    "        centered = error_points - error_points.mean(axis=0)\n",
    "        U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "        stats['singular_values'] = S\n",
    "        stats['condition_number'] = S.max() / (S.min() + 1e-10)\n",
    "        stats['principal_directions'] = Vt\n",
    "        \n",
    "        # Bounding box\n",
    "        stats['bbox_min'] = error_points.min(axis=0)\n",
    "        stats['bbox_max'] = error_points.max(axis=0)\n",
    "        stats['bbox_volume'] = np.prod(stats['bbox_max'] - stats['bbox_min'])\n",
    "        \n",
    "        # Convex hull volume (only for low dims)\n",
    "        if self.dims <= 8 and len(error_points) > self.dims + 1:\n",
    "            try:\n",
    "                hull = ConvexHull(error_points)\n",
    "                stats['convex_hull_volume'] = hull.volume\n",
    "            except:\n",
    "                stats['convex_hull_volume'] = None\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "def run_linear_only_experiment(dims=2, n_layers=4, bits=8):\n",
    "    \"\"\"\n",
    "    Track error shape through linear layers only (no nonlinearities).\n",
    "    \"\"\"\n",
    "    tracker = ErrorShapeTracker(dims=dims, bits=bits)\n",
    "    \n",
    "    # Initialize error shape\n",
    "    error_points = tracker.initial_error_hypercube(n_samples=2000)\n",
    "    \n",
    "    history = [{\n",
    "        'layer': 'input',\n",
    "        'points': error_points.copy(),\n",
    "        'stats': tracker.measure_shape(error_points)\n",
    "    }]\n",
    "    \n",
    "    print(f\"Layer input: {len(error_points)} points, dims={error_points.shape[1]}\")\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        # Random weight matrix (with some structure)\n",
    "        W = np.random.randn(dims, dims) * 0.5\n",
    "        W = W + np.eye(dims) * 0.5  # Add some identity to prevent collapse\n",
    "        \n",
    "        # Quantize W\n",
    "        W_quant = np.round(W * (2**bits)) / (2**bits)\n",
    "        \n",
    "        # The error in W also contributes\n",
    "        W_error = W - W_quant\n",
    "        \n",
    "        # Transform error region\n",
    "        error_points = tracker.transform_linear(error_points, W_quant)\n",
    "        \n",
    "        # Add new quantization error (Minkowski sum)\n",
    "        error_points = tracker.add_quantization_error(error_points)\n",
    "        \n",
    "        stats = tracker.measure_shape(error_points)\n",
    "        history.append({\n",
    "            'layer': f'linear_{i}',\n",
    "            'points': error_points.copy(),\n",
    "            'stats': stats,\n",
    "            'W': W_quant,\n",
    "            'W_spectral_norm': np.linalg.norm(W_quant, ord=2)\n",
    "        })\n",
    "        \n",
    "        print(f\"Layer {i}: spectral_norm={np.linalg.norm(W_quant, ord=2):.3f}, \"\n",
    "              f\"condition={stats['condition_number']:.2f}, \"\n",
    "              f\"log_vol={stats['log_volume']:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def run_with_nonlinearities_experiment(dims=2, n_layers=4, bits=8):\n",
    "    \"\"\"\n",
    "    Track error shape with ReLU and saturation.\n",
    "    \"\"\"\n",
    "    tracker = ErrorShapeTracker(dims=dims, bits=bits)\n",
    "    qmin, qmax = -1.0, 1.0  # Activation range\n",
    "    \n",
    "    # Initialize\n",
    "    error_points = tracker.initial_error_hypercube(n_samples=2000)\n",
    "    activation_center = np.random.uniform(0.2, 0.8, size=dims)  # Start positive\n",
    "    \n",
    "    history = [{\n",
    "        'layer': 'input',\n",
    "        'points': error_points.copy(),\n",
    "        'center': activation_center.copy(),\n",
    "        'stats': tracker.measure_shape(error_points)\n",
    "    }]\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        # Weight matrix\n",
    "        W = np.random.randn(dims, dims) * 0.5 + np.eye(dims) * 0.3\n",
    "        W_quant = np.round(W * (2**bits)) / (2**bits)\n",
    "        \n",
    "        # Linear transform\n",
    "        error_points = tracker.transform_linear(error_points, W_quant)\n",
    "        activation_center = W_quant @ activation_center\n",
    "        \n",
    "        # Add quantization error\n",
    "        error_points = tracker.add_quantization_error(error_points)\n",
    "        \n",
    "        stats_pre_relu = tracker.measure_shape(error_points)\n",
    "        n_points_pre = len(error_points)\n",
    "        \n",
    "        # Apply ReLU\n",
    "        error_points, activation_center = tracker.apply_relu_to_error(\n",
    "            error_points, activation_center\n",
    "        )\n",
    "        \n",
    "        # Apply saturation\n",
    "        error_points, activation_center = tracker.apply_saturation(\n",
    "            error_points, activation_center, qmin, qmax\n",
    "        )\n",
    "        \n",
    "        # Remove points that collapsed to the same value (degenerate)\n",
    "        # This represents information loss\n",
    "        unique_points = np.unique(np.round(error_points, decimals=6), axis=0)\n",
    "        collapse_ratio = 1 - len(unique_points) / n_points_pre\n",
    "        \n",
    "        stats_post = tracker.measure_shape(error_points)\n",
    "        \n",
    "        history.append({\n",
    "            'layer': f'layer_{i}',\n",
    "            'points': error_points.copy(),\n",
    "            'center': activation_center.copy(),\n",
    "            'stats_pre_nonlin': stats_pre_relu,\n",
    "            'stats_post_nonlin': stats_post,\n",
    "            'collapse_ratio': collapse_ratio,\n",
    "            'W_spectral_norm': np.linalg.norm(W_quant, ord=2)\n",
    "        })\n",
    "        \n",
    "        print(f\"Layer {i}: \"\n",
    "              f\"spectral_norm={np.linalg.norm(W_quant, ord=2):.3f}, \"\n",
    "              f\"log_vol_pre={stats_pre_relu['log_volume']:.3f}, \"\n",
    "              f\"log_vol_post={stats_post['log_volume']:.3f}, \"\n",
    "              f\"collapsed={collapse_ratio*100:.1f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def visualize_2d_evolution(history):\n",
    "    \"\"\"\n",
    "    For 2D case, visualize the error shape at each layer.\n",
    "    \"\"\"\n",
    "    n_layers = len(history)\n",
    "    fig, axes = plt.subplots(2, (n_layers + 1) // 2, figsize=(4 * ((n_layers + 1) // 2), 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, h in enumerate(history):\n",
    "        ax = axes[i]\n",
    "        points = h['points']\n",
    "        \n",
    "        ax.scatter(points[:, 0], points[:, 1], alpha=0.3, s=1)\n",
    "        \n",
    "        # Draw convex hull\n",
    "        if len(points) > 3:\n",
    "            try:\n",
    "                hull = ConvexHull(points)\n",
    "                for simplex in hull.simplices:\n",
    "                    ax.plot(points[simplex, 0], points[simplex, 1], 'r-', linewidth=1)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Draw principal axes\n",
    "        stats = h.get('stats') or h.get('stats_post_nonlin')\n",
    "        if stats and 'principal_directions' in stats:\n",
    "            center = points.mean(axis=0)\n",
    "            for j, (s, v) in enumerate(zip(stats['singular_values'][:2], \n",
    "                                           stats['principal_directions'][:2])):\n",
    "                ax.arrow(center[0], center[1], v[0]*s*0.5, v[1]*s*0.5, \n",
    "                        head_width=0.02, color=['blue', 'green'][j], linewidth=2)\n",
    "        \n",
    "        title = h['layer']\n",
    "        if 'collapse_ratio' in h:\n",
    "            title += f\"\\ncollapsed: {h['collapse_ratio']*100:.1f}%\"\n",
    "        ax.set_title(title)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(0, color='k', linewidth=0.5)\n",
    "        ax.axvline(0, color='k', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_shape_evolution_stats(history):\n",
    "    \"\"\"\n",
    "    Plot scalar statistics across layers (works for any dimension).\n",
    "    \"\"\"\n",
    "    layers = [h['layer'] for h in history]\n",
    "    \n",
    "    # Extract stats\n",
    "    log_vols = []\n",
    "    conditions = []\n",
    "    spectral_norms = []\n",
    "    collapse_ratios = []\n",
    "    \n",
    "    for h in history:\n",
    "        stats = h.get('stats') or h.get('stats_post_nonlin')\n",
    "        log_vols.append(stats['log_volume'] if stats else 0)\n",
    "        conditions.append(stats['condition_number'] if stats else 1)\n",
    "        spectral_norms.append(h.get('W_spectral_norm', 1))\n",
    "        collapse_ratios.append(h.get('collapse_ratio', 0))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(log_vols, 'o-', linewidth=2, markersize=8)\n",
    "    ax.set_xticks(range(len(layers)))\n",
    "    ax.set_xticklabels(layers, rotation=45)\n",
    "    ax.set_ylabel('log(Volume)')\n",
    "    ax.set_title('Error Region Volume (log scale)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(conditions, 's-', linewidth=2, markersize=8, color='orange')\n",
    "    ax.set_xticks(range(len(layers)))\n",
    "    ax.set_xticklabels(layers, rotation=45)\n",
    "    ax.set_ylabel('Condition Number')\n",
    "    ax.set_title('Shape Elongation\\n(high = stretched in one direction)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    ax.bar(range(1, len(spectral_norms)), spectral_norms[1:], color='steelblue')\n",
    "    ax.set_xticks(range(1, len(layers)))\n",
    "    ax.set_xticklabels(layers[1:], rotation=45)\n",
    "    ax.set_ylabel('Spectral Norm ||W||')\n",
    "    ax.set_title('Weight Matrix Spectral Norms\\n(error amplification factor)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.bar(range(len(collapse_ratios)), collapse_ratios, color='red', alpha=0.7)\n",
    "    ax.set_xticks(range(len(layers)))\n",
    "    ax.set_xticklabels(layers, rotation=45)\n",
    "    ax.set_ylabel('Collapse Ratio')\n",
    "    ax.set_title('Information Loss from Nonlinearities\\n(fraction of points collapsed)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LINEAR ONLY (2D)\")\n",
    "    print(\"=\" * 60)\n",
    "    history_linear = run_linear_only_experiment(dims=2, n_layers=5, bits=8)\n",
    "    \n",
    "    fig = visualize_2d_evolution(history_linear)\n",
    "    plt.savefig('plots/error_shape_linear_2d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"WITH NONLINEARITIES (2D)\")\n",
    "    print(\"=\" * 60)\n",
    "    history_nonlin = run_with_nonlinearities_experiment(dims=2, n_layers=5, bits=8)\n",
    "    \n",
    "    fig = visualize_2d_evolution(history_nonlin)\n",
    "    plt.savefig('plots/error_shape_nonlin_2d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = visualize_shape_evolution_stats(history_nonlin)\n",
    "    plt.savefig('plots/error_shape_stats.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HIGHER DIMENSIONAL (10D)\")\n",
    "    print(\"=\" * 60)\n",
    "    history_10d = run_with_nonlinearities_experiment(dims=10, n_layers=5, bits=8)\n",
    "    \n",
    "    fig = visualize_shape_evolution_stats(history_10d)\n",
    "    plt.savefig('plots/error_shape_stats_10d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d01eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-aware-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
